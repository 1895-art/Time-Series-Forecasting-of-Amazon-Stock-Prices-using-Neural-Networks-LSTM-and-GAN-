{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Untitled0.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"KGrrt16a7kR2","colab_type":"code","outputId":"564c7ad1-ca9b-4622-ccec-504ab1a03d22","executionInfo":{"status":"ok","timestamp":1564090769967,"user_tz":240,"elapsed":254,"user":{"displayName":"nupur deshpande","photoUrl":"https://lh4.googleusercontent.com/-AQjf2oEW7kE/AAAAAAAAAAI/AAAAAAAAA1Y/ZFcRZICzWa0/s64/photo.jpg","userId":"04753088851981756999"}},"colab":{"base_uri":"https://localhost:8080/","height":33}},"source":["import tensorflow as tf\n","device_name = tf.test.gpu_device_name()\n","if device_name != '/device:GPU:0':\n","  raise SystemError('GPU device not found')\n","print('Found GPU at: {}'.format(device_name))"],"execution_count":66,"outputs":[{"output_type":"stream","text":["Found GPU at: /device:GPU:0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"SACvOFk574rz","colab_type":"code","outputId":"9b7808f5-838a-4dbb-c022-f6fd0b80dc02","executionInfo":{"status":"ok","timestamp":1564090774093,"user_tz":240,"elapsed":2644,"user":{"displayName":"nupur deshpande","photoUrl":"https://lh4.googleusercontent.com/-AQjf2oEW7kE/AAAAAAAAAAI/AAAAAAAAA1Y/ZFcRZICzWa0/s64/photo.jpg","userId":"04753088851981756999"}},"colab":{"base_uri":"https://localhost:8080/","height":33}},"source":["from google.colab import drive\n","\n","# This will prompt for authorization.\n","# drive.mount('/content/drive')\n","drive.mount(\"/content/drive\", force_remount=True)"],"execution_count":67,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"lzSA6-II8nEg","colab_type":"code","colab":{}},"source":["googlepath = \"/content/drive/My Drive/Colab Notebooks/SeniorDesignGAN/MiloGAN/\"\n","\n","# Setting the Training Amount\n","TRAINING_AMOUNT = 50000 # low to test for now\n","SAVE_STEPS_AMOUNT = 10000 # testing for now\n","PCT_CHANGE_AMOUNT = 5 # just want to see up down trends\n","HISTORICAL_DAYS_AMOUNT = 20\n","DAYS_AHEAD = 5"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"7_HY8l1y8WvP","colab_type":"code","colab":{}},"source":["#Getting the Data"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"1tCCJqT_9NVW","colab_type":"code","outputId":"df28bbb0-b9db-43dc-e873-0e30cb09904b","executionInfo":{"status":"ok","timestamp":1564090805504,"user_tz":240,"elapsed":27938,"user":{"displayName":"nupur deshpande","photoUrl":"https://lh4.googleusercontent.com/-AQjf2oEW7kE/AAAAAAAAAAI/AAAAAAAAA1Y/ZFcRZICzWa0/s64/photo.jpg","userId":"04753088851981756999"}},"colab":{"base_uri":"https://localhost:8080/","height":50}},"source":["'''\n","Downloads stock data from alphavantage\n","'''\n","import pandas as pd \n","import os\n","import time\n","import urllib\n","import json\n","import csv\n","import requests\n","import io\n","from pathlib import Path\n","import random\n","\n","ALPHA_VANTAGE_KEY = \"CDRQTN7OC8V8TPKD\"\n","\n","'''\n","Note should have companylist.csv in the directory with this file.\n","'''\n","\n","'''\n","Saves data to a file\n","'''\n","def save(googlepath, stock_csv, output_dir, filename):\n","    try:\n","        #the output dir may not exist\n","        if not os.path.exists(output_dir):\n","            os.makedirs(output_dir)\n","    except Exception as ex:\n","        print('Could not create output dir')\n","        print(ex)\n","        return\n","    filepath = os.path.join(googlepath, output_dir, filename)\n","    try:\n","#         print(stock_csv)\n","        df = stock_csv\n","        df = df.sort_values(by='timestamp')  \n","#         print(df)\n","        df.to_csv(filepath, index=False)\n","    except Exception as ex:\n","        print('Could not open file {} to write data'.format(filepath))\n","        print(ex)\n","\n","\n","def try_download(symbol):\n","    try:\n","        # Keep call frequency below threshold \n","        time.sleep(12)    \n","        url = 'https://www.alphavantage.co/query?function=TIME_SERIES_DAILY_ADJUSTED&symbol={}&apikey={}&datatype=csv&outputsize=full'.format(symbol, ALPHA_VANTAGE_KEY)\n","        c = pd.read_csv(url)\n","        # getting rid of some columns won't look at for now\n","        c = c.drop(['split_coefficient', 'dividend_amount', 'adjusted_close'], axis=1)\n","        return c, True\n","    except Exception as ex:\n","        print(ex)\n","        return None, None\n","\n","\n","\n","#Given a stock symbol (aka 'tsla') will download and save the data to the\n","#output dir as a csv \n","\n","def download_symbol(symbol, output_dir, retry_count=4):\n","\n","    stock_csv, didPass = try_download(symbol)\n","    if didPass:\n","        save(googlepath, stock_csv, output_dir, '{}.csv'.format(symbol))\n","    else:\n","        print('Failed to download {}'.format(symbol))\n","\n","df = pd.read_csv(f\"{googlepath}companylist.csv\")\n","# df = df.sort_values(by=['MarketCap'], ascending=False)\n","# Top 30 Companies\n","# df = df[:30]\n","for symbol in df.Symbol:\n","    my_file = Path(f\"{googlepath}stock_data/{symbol}.csv\")  # check if already downloaded\n","#     print(my_file.exists())\n","    if not my_file.exists():\n","        print('Downloading {}'.format(symbol))\n","        download_symbol(symbol, 'stock_data')\n","    else:\n","        print(f\"Already downloaded {symbol}\")\n"],"execution_count":70,"outputs":[{"output_type":"stream","text":["Downloading AMZN\n","Downloading AAPL\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"HIy6WWcg-M5q","colab_type":"code","colab":{}},"source":["#plot confusion matrices\n","import itertools\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from sklearn.metrics import confusion_matrix\n","\n","\n","def plot_confusion_matrix(cm, classes,\n","                          normalize=False,\n","                          title='Confusion matrix',\n","                          cmap=plt.cm.Blues):\n","    \"\"\"\n","    This function prints and plots the confusion matrix.\n","    Normalization can be applied by setting `normalize=True`.\n","    \"\"\"\n","    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n","    plt.title(title)\n","    plt.colorbar()\n","    tick_marks = np.arange(len(classes))\n","    plt.xticks(tick_marks, classes, rotation=45)\n","    plt.yticks(tick_marks, classes)\n","\n","    if normalize:\n","        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n","        print(\"Normalized confusion matrix\")\n","    else:\n","        print('Confusion matrix, without normalization')\n","\n","    print(cm)\n","\n","    thresh = cm.max() / 2.\n","    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n","        plt.text(j, i, cm[i, j],\n","                 horizontalalignment=\"center\",\n","                 color=\"black\")\n","\n","    plt.tight_layout()\n","    plt.ylabel('True label')\n","    plt.xlabel('Predicted label')\n","    plt.show()\n","    "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"k070Rvz8Qirt","colab_type":"code","colab":{}},"source":["import tensorflow as tf\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import os\n","\n","SEED = 42\n","tf.set_random_seed(SEED)\n","\n","class GAN():\n","\n","    def sample_Z(self, batch_size, n):\n","        return np.random.uniform(-1., 1., size=(batch_size, n))\n","\n","    def __init__(self, num_features, num_historical_days, generator_input_size=200, is_train=True):\n","        def get_batch_norm_with_global_normalization_vars(size):\n","            v = tf.Variable(tf.ones([size]), dtype=tf.float32)\n","            m = tf.Variable(tf.ones([size]), dtype=tf.float32)\n","            beta = tf.Variable(tf.ones([size]), dtype=tf.float32)\n","            gamma = tf.Variable(tf.ones([size]), dtype=tf.float32)\n","            return v, m, beta, gamma\n","\n","        self.X = tf.placeholder(tf.float32, shape=[None, num_historical_days, num_features])\n","        X = tf.reshape(self.X, [-1, num_historical_days, 1, num_features])\n","        self.Z = tf.placeholder(tf.float32, shape=[None, generator_input_size])\n","\n","        generator_output_size = num_features*num_historical_days\n","        with tf.variable_scope(\"generator\"):\n","            W1 = tf.Variable(tf.truncated_normal([generator_input_size, generator_output_size*10]))\n","            b1 = tf.Variable(tf.truncated_normal([generator_output_size*10]))\n","\n","            h1 = tf.nn.sigmoid(tf.matmul(self.Z, W1) + b1)\n","\n","            # v1, m1, beta1, gamma1 = get_batch_norm_with_global_normalization_vars(generator_output_size*10)\n","            # h1 = tf.nn.batch_norm_with_global_normalization(h1, v1, m1,\n","            #         beta1, gamma1, variance_epsilon=0.000001, scale_after_normalization=False)\n","\n","            W2 = tf.Variable(tf.truncated_normal([generator_output_size*10, generator_output_size*5]))\n","            b2 = tf.Variable(tf.truncated_normal([generator_output_size*5]))\n","\n","            h2 = tf.nn.sigmoid(tf.matmul(h1, W2) + b2)\n","\n","            # v2, m2, beta2, gamma2 = get_batch_norm_with_global_normalization_vars(generator_output_size*5)\n","            # h2 = tf.nn.batch_norm_with_global_normalization(h2, v2, m2,\n","            #         beta2, gamma2, variance_epsilon=0.000001, scale_after_normalization=False)\n","\n","\n","            W3 = tf.Variable(tf.truncated_normal([generator_output_size*5, generator_output_size]))\n","            b3 = tf.Variable(tf.truncated_normal([generator_output_size]))\n","\n","            g_log_prob = tf.matmul(h2, W3) + b3\n","            g_log_prob = tf.reshape(g_log_prob, [-1, num_historical_days, 1, num_features])\n","            self.gen_data = tf.reshape(g_log_prob, [-1, num_historical_days, num_features])\n","            #g_log_prob = g_log_prob / tf.reshape(tf.reduce_max(g_log_prob, axis=1), [-1, 1, num_features, 1])\n","            #g_prob = tf.nn.sigmoid(g_log_prob)\n","\n","            theta_G = [W1, b1, W2, b2, W3, b3]\n","\n","\n","\n","        with tf.variable_scope(\"discriminator\"):\n","            #[filter_height, filter_width, in_channels, out_channels]\n","            k1 = tf.Variable(tf.truncated_normal([3, 1, num_features, 32],\n","                stddev=0.1,seed=SEED, dtype=tf.float32))\n","            b1 = tf.Variable(tf.zeros([32], dtype=tf.float32))\n","\n","            v1, m1, beta1, gamma1 = get_batch_norm_with_global_normalization_vars(32)\n","\n","            k2 = tf.Variable(tf.truncated_normal([3, 1, 32, 64],\n","                stddev=0.1,seed=SEED, dtype=tf.float32))\n","            b2 = tf.Variable(tf.zeros([64], dtype=tf.float32))\n","\n","            v2, m2, beta2, gamma2 = get_batch_norm_with_global_normalization_vars(64)\n","\n","            k3 = tf.Variable(tf.truncated_normal([3, 1, 64, 128],\n","                stddev=0.1,seed=SEED, dtype=tf.float32))\n","            b3 = tf.Variable(tf.zeros([128], dtype=tf.float32))\n","\n","            v3, m3, beta3, gamma3 = get_batch_norm_with_global_normalization_vars(128)\n","\n","            W1 = tf.Variable(tf.truncated_normal([18*1*128, 128]))\n","            b4 = tf.Variable(tf.truncated_normal([128]))\n","\n","            v4, m4, beta4, gamma4 = get_batch_norm_with_global_normalization_vars(128)\n","\n","            W2 = tf.Variable(tf.truncated_normal([128, 1]))\n","\n","            theta_D = [k1, b1, k2, b2, k3, b3, W1, b4, W2]\n","\n","        def discriminator(X):\n","            conv = tf.nn.conv2d(X,k1,strides=[1, 1, 1, 1],padding='SAME')\n","            relu = tf.nn.relu(tf.nn.bias_add(conv, b1))\n","            pool = relu\n","            # pool = tf.nn.avg_pool(relu, ksize=[1, 2, 1, 1], strides=[1, 2, 1, 1], padding='SAME')\n","            if is_train:\n","                pool = tf.nn.dropout(pool, keep_prob = 0.8)\n","            # pool = tf.nn.batch_norm_with_global_normalization(pool, v1, m1,\n","            #         beta1, gamma1, variance_epsilon=0.000001, scale_after_normalization=False)\n","            print(pool)\n","\n","            conv = tf.nn.conv2d(pool, k2,strides=[1, 1, 1, 1],padding='SAME')\n","            relu = tf.nn.relu(tf.nn.bias_add(conv, b2))\n","            pool = relu\n","            #pool = tf.nn.avg_pool(relu, ksize=[1, 2, 1, 1], strides=[1, 2, 1, 1], padding='SAME')\n","            if is_train:\n","                pool = tf.nn.dropout(pool, keep_prob = 0.8)\n","            # pool = tf.nn.batch_norm_with_global_normalization(pool, v2, m2,\n","            #         beta2, gamma2, variance_epsilon=0.000001, scale_after_normalization=False)\n","            print(pool)\n","\n","            conv = tf.nn.conv2d(pool, k3, strides=[1, 1, 1, 1], padding='VALID')\n","            relu = tf.nn.relu(tf.nn.bias_add(conv, b3))\n","            if is_train:\n","                relu = tf.nn.dropout(relu, keep_prob=0.8)\n","            # relu = tf.nn.batch_norm_with_global_normalization(relu, v3, m3,\n","            #         beta3, gamma3, variance_epsilon=0.000001, scale_after_normalization=False)\n","            print(relu)\n","\n","\n","            flattened_convolution_size = int(relu.shape[1]) * int(relu.shape[2]) * int(relu.shape[3])\n","            print(flattened_convolution_size)\n","            flattened_convolution = features = tf.reshape(relu, [-1, flattened_convolution_size])\n","\n","            if is_train:\n","                flattened_convolution =  tf.nn.dropout(flattened_convolution, keep_prob=0.8)\n","\n","            h1 = tf.nn.relu(tf.matmul(flattened_convolution, W1) + b4)\n","\n","            # h1 = tf.nn.batch_norm_with_global_normalization(h1, v4, m4,\n","            #         beta4, gamma4, variance_epsilon=0.000001, scale_after_normalization=False)\n","\n","            D_logit = tf.matmul(h1, W2)\n","            D_prob = tf.nn.sigmoid(D_logit)\n","            return D_prob, D_logit, features\n","\n","        D_real, D_logit_real, self.features = discriminator(X)\n","        D_fake, D_logit_fake, _ = discriminator(g_log_prob)\n","\n","\n","        D_loss_real = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=D_logit_real, labels=tf.ones_like(D_logit_real)))\n","        D_loss_fake = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=D_logit_fake, labels=tf.zeros_like(D_logit_fake)))\n","        self.D_l2_loss = (0.0001 * tf.add_n([tf.nn.l2_loss(t) for t in theta_D]) / len(theta_D))\n","        self.D_loss = D_loss_real + D_loss_fake + self.D_l2_loss\n","        self.G_l2_loss = (0.00001 * tf.add_n([tf.nn.l2_loss(t) for t in theta_G]) / len(theta_G))\n","        self.G_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=D_logit_fake, labels=tf.ones_like(D_logit_fake))) + self.G_l2_loss\n","\n","\n","        self.D_solver = tf.train.AdamOptimizer(learning_rate=0.0001).minimize(self.D_loss, var_list=theta_D)\n","        self.G_solver = tf.train.AdamOptimizer(learning_rate=0.000055).minimize(self.G_loss, var_list=theta_G)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"k4-qKPvQQ4zD","colab_type":"code","outputId":"b7e5b04e-bf91-41a3-9d06-c6fc58a393e5","executionInfo":{"status":"ok","timestamp":1564091245140,"user_tz":240,"elapsed":391327,"user":{"displayName":"nupur deshpande","photoUrl":"https://lh4.googleusercontent.com/-AQjf2oEW7kE/AAAAAAAAAAI/AAAAAAAAA1Y/ZFcRZICzWa0/s64/photo.jpg","userId":"04753088851981756999"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["#training the GAN\n","import os\n","import pandas as pd\n","# from gan import GAN\n","import random\n","import tensorflow as tf\n","\n","random.seed(42)\n","class TrainGan:\n","\n","    def __init__(self, num_historical_days, batch_size=128):\n","        self.batch_size = batch_size\n","        self.data = []\n","#         files = [os.path.join('./stock_data', f) for f in os.listdir('./stock_data')]\n","\n","        # Google Drive Method\n","        files = [f\"{googlepath}stock_data/{f}\" for f in os.listdir(f\"{googlepath}stock_data\")]\n","#         print(files)\n","      \n","        for file in files:\n","            print(file)\n","            #Read in file -- note that parse_dates will be need later\n","            df = pd.read_csv(file, index_col='timestamp', parse_dates=True)\n","            df = df[['open','high','low','close','volume']]\n","            # #Create new index with missing days\n","            # idx = pd.date_range(df.index[-1], df.index[0])\n","            # #Reindex and fill the missing day with the value from the day before\n","            # df = df.reindex(idx, method='bfill').sort_index(ascending=False)\n","            #Normilize using a of size num_historical_days\n","            df = ((df -\n","            df.rolling(num_historical_days).mean().shift(-num_historical_days))\n","            /(df.rolling(num_historical_days).max().shift(-num_historical_days)\n","            -df.rolling(num_historical_days).min().shift(-num_historical_days)))\n","            #Drop the last 10 day that we don't have data for\n","            df = df.dropna()\n","            #Hold out the last year of trading for testing\n","            #Padding to keep labels from bleeding\n","            df = df[400:]\n","            #This may not create good samples if num_historical_days is a\n","            #mutliple of 7\n","            for i in range(num_historical_days, len(df), num_historical_days):\n","                self.data.append(df.values[i-num_historical_days:i])\n","\n","        self.gan = GAN(num_features=5, num_historical_days=num_historical_days,\n","                        generator_input_size=200)\n","\n","    def random_batch(self, batch_size=128):\n","        batch = []\n","        while True:\n","            batch.append(random.choice(self.data))\n","            if (len(batch) == batch_size):\n","                yield batch\n","                batch = []\n","\n","    def train(self, print_steps=100, display_data=100, save_steps=SAVE_STEPS_AMOUNT):\n","        if not os.path.exists(f'{googlepath}models'):\n","            os.makedirs(f'{googlepath}models')\n","        sess = tf.Session()\n","        \n","        G_loss = 0\n","        D_loss = 0\n","        G_l2_loss = 0\n","        D_l2_loss = 0\n","        sess.run(tf.global_variables_initializer())\n","        saver = tf.train.Saver()\n","        currentStep = \"0\"\n","        \n","        g_loss_array = []\n","        d_loss_array = []\n","        \n","        if os.path.exists(f'{googlepath}models/checkpoint'):\n","                with open(f'{googlepath}models/checkpoint', 'rb') as f:\n","                    model_name = next(f).split('\"'.encode())[1]\n","                filename = \"{}models/{}\".format(googlepath, model_name.decode())\n","                currentStep = filename.split(\"-\")[1]\n","                new_saver = tf.train.import_meta_graph('{}.meta'.format(filename))\n","                new_saver.restore(sess, \"{}\".format(filename))\n","\n","        for i, X in enumerate(self.random_batch(self.batch_size)):\n","\n","            \n","            \n","            \n","            if i % 1 == 0:\n","                _, D_loss_curr, D_l2_loss_curr = sess.run([self.gan.D_solver, self.gan.D_loss, self.gan.D_l2_loss], feed_dict=\n","                        {self.gan.X:X, self.gan.Z:self.gan.sample_Z(self.batch_size, 200)})\n","                D_loss += D_loss_curr\n","                D_l2_loss += D_l2_loss_curr\n","            if i % 1 == 0:\n","                _, G_loss_curr, G_l2_loss_curr = sess.run([self.gan.G_solver, self.gan.G_loss, self.gan.G_l2_loss],\n","                        feed_dict={self.gan.Z:self.gan.sample_Z(self.batch_size, 200)})\n","                G_loss += G_loss_curr\n","                G_l2_loss += G_l2_loss_curr\n","                \n","            g_loss_array.append(G_loss_curr - G_l2_loss)\n","            d_loss_array.append(D_loss_curr - D_l2_loss)\n","            \n","            \n","            if (i+1) % print_steps == 0:\n","                print('Step={} D_loss={}, G_loss={}'.format(i + int(currentStep), D_loss/print_steps - D_l2_loss/print_steps, G_loss/print_steps - G_l2_loss/print_steps))\n","                #print('D_l2_loss = {} G_l2_loss={}'.format(D_l2_loss/print_steps, G_l2_loss/print_steps))\n","                G_loss = 0\n","                D_loss = 0\n","                G_l2_loss = 0\n","                D_l2_loss = 0\n","            if (i+1) % save_steps == 0:\n","                saver.save(sess, f'{googlepath}/models/gan.ckpt', i + int(currentStep))\n","            \n","            # end training at training_amount epochs\n","            if ((i + int(currentStep)) > TRAINING_AMOUNT):\n","                \n","                print(\"Reached {} epochs for GAN\".format(i + int(currentStep)))\n","                sess.close()\n","                \n","                axisX = np.arange(0,len(g_loss_array),1)\n","                plt.plot(axisX, g_loss_array, label='generator loss')\n","                plt.plot(axisX, d_loss_array, label='discriminator loss')\n","                plt.legend()\n","                plt.title('generator and discriminator loss')\n","                plt.show()\n","                \n","                break\n","\n","            # if (i+1) % display_data == 0:\n","            #     print('Generated Data')\n","            #     print(sess.run(self.gan.gen_data, feed_dict={self.gan.Z:self.gan.sample_Z(1, 200)}))\n","            #     print('Real Data')\n","            #     print(X[0])\n","\n","\n","# if __name__ == '__main__':\n","tf.reset_default_graph()\n","gan = TrainGan(HISTORICAL_DAYS_AMOUNT, 128)\n","gan.train()"],"execution_count":73,"outputs":[{"output_type":"stream","text":["/content/drive/My Drive/Colab Notebooks/SeniorDesignGAN/MiloGAN/stock_data/AMZN.csv\n","/content/drive/My Drive/Colab Notebooks/SeniorDesignGAN/MiloGAN/stock_data/AAPL.csv\n","Tensor(\"dropout/mul_1:0\", shape=(?, 20, 1, 32), dtype=float32)\n","Tensor(\"dropout_1/mul_1:0\", shape=(?, 20, 1, 64), dtype=float32)\n","Tensor(\"dropout_2/mul_1:0\", shape=(?, 18, 1, 128), dtype=float32)\n","2304\n","Tensor(\"dropout_4/mul_1:0\", shape=(?, 20, 1, 32), dtype=float32)\n","Tensor(\"dropout_5/mul_1:0\", shape=(?, 20, 1, 64), dtype=float32)\n","Tensor(\"dropout_6/mul_1:0\", shape=(?, 18, 1, 128), dtype=float32)\n","2304\n","Step=99 D_loss=18.938402512073516, G_loss=766.5629239159822\n","Step=199 D_loss=4.296135082244874, G_loss=736.2474687924981\n","Step=299 D_loss=2.1010798990726474, G_loss=710.6778854677082\n","Step=399 D_loss=1.1316085731983188, G_loss=673.101349737048\n","Step=499 D_loss=1.0086108112335204, G_loss=653.0465836727619\n","Step=599 D_loss=0.9443923807144163, G_loss=639.0484749719501\n","Step=699 D_loss=0.7062720191478729, G_loss=648.9698222965002\n","Step=799 D_loss=0.5236468029022217, G_loss=640.634911389053\n","Step=899 D_loss=0.40585802197456355, G_loss=655.9977617111803\n","Step=999 D_loss=0.2889783239364625, G_loss=669.8019690206647\n","Step=1099 D_loss=0.3264579677581787, G_loss=688.2358854573965\n","Step=1199 D_loss=0.15227504014968862, G_loss=712.4857186779379\n","Step=1299 D_loss=0.15599569082260123, G_loss=691.9988962742686\n","Step=1399 D_loss=0.10993398070335392, G_loss=696.8119847875834\n","Step=1499 D_loss=0.1574604868888856, G_loss=700.8095920559764\n","Step=1599 D_loss=0.10953044056892391, G_loss=718.6650937032699\n","Step=1699 D_loss=0.10473617434501659, G_loss=728.9580990880727\n","Step=1799 D_loss=0.07397578239440916, G_loss=727.2791827756166\n","Step=1899 D_loss=0.15505398988723762, G_loss=700.2794566738605\n","Step=1999 D_loss=0.053540689945220876, G_loss=736.8001477387547\n","Step=2099 D_loss=0.07958808422088604, G_loss=728.7874432203174\n","Step=2199 D_loss=0.13063920974731436, G_loss=725.7168947300314\n","Step=2299 D_loss=0.11791766405105597, G_loss=701.1627981367708\n","Step=2399 D_loss=0.15853978991508488, G_loss=764.0851395320892\n","Step=2499 D_loss=0.12845052838325488, G_loss=685.1845959404111\n","Step=2599 D_loss=0.21653019428253173, G_loss=699.23023270607\n","Step=2699 D_loss=0.2681520640850068, G_loss=685.0723531761765\n","Step=2799 D_loss=0.07822263598442092, G_loss=673.9799268147349\n","Step=2899 D_loss=0.12179451107978823, G_loss=640.7271762186289\n","Step=2999 D_loss=0.13028664588928218, G_loss=600.958370885551\n","Step=3099 D_loss=0.19130797863006577, G_loss=555.1973092603683\n","Step=3199 D_loss=0.14403699159622185, G_loss=533.0775874444843\n","Step=3299 D_loss=0.21258293271064743, G_loss=488.13990022450685\n","Step=3399 D_loss=0.4221708452701569, G_loss=423.1563803604245\n","Step=3499 D_loss=0.5671156001091004, G_loss=389.5129234728217\n","Step=3599 D_loss=0.42045866608619686, G_loss=363.73205646783117\n","Step=3699 D_loss=0.5282859253883363, G_loss=359.0764731660485\n","Step=3799 D_loss=0.8036527419090271, G_loss=314.88030287444593\n","Step=3899 D_loss=0.9813853991031647, G_loss=280.1364270618558\n","Step=3999 D_loss=0.7703526115417481, G_loss=220.9749648270011\n","Step=4099 D_loss=0.632391105890274, G_loss=193.37595436394213\n","Step=4199 D_loss=0.4185970830917358, G_loss=171.461988978982\n","Step=4299 D_loss=0.5082120621204376, G_loss=143.9425900620222\n","Step=4399 D_loss=0.43421312928199773, G_loss=126.36621915757655\n","Step=4499 D_loss=0.5269295394420623, G_loss=102.98833870202303\n","Step=4599 D_loss=0.3667387330532075, G_loss=94.61488351017236\n","Step=4699 D_loss=0.23347719669342037, G_loss=89.81154131323099\n","Step=4799 D_loss=0.2399594283103943, G_loss=80.47828148186207\n","Step=4899 D_loss=0.32581158638000485, G_loss=70.7319988283515\n","Step=4999 D_loss=0.22422497272491437, G_loss=66.06128502100705\n","Step=5099 D_loss=0.21176603436470032, G_loss=62.35026930481195\n","Step=5199 D_loss=0.18234689474105825, G_loss=58.61630676209927\n","Step=5299 D_loss=0.17821869492530817, G_loss=53.70207858681678\n","Step=5399 D_loss=0.12595008730888368, G_loss=51.90239357739687\n","Step=5499 D_loss=0.15135288953781134, G_loss=50.829170714914795\n","Step=5599 D_loss=0.10342615246772757, G_loss=47.591841354966164\n","Step=5699 D_loss=0.06265819549560558, G_loss=48.5216320720315\n","Step=5799 D_loss=0.07537703871726986, G_loss=47.74967734217643\n","Step=5899 D_loss=0.0786401629447937, G_loss=44.999039899706844\n","Step=5999 D_loss=0.05164069414138783, G_loss=43.623042488396166\n","Step=6099 D_loss=0.05060348629951483, G_loss=43.58821940004826\n","Step=6199 D_loss=0.0454485273361207, G_loss=45.51568178027868\n","Step=6299 D_loss=0.0466648375988008, G_loss=44.672813853621484\n","Step=6399 D_loss=0.03456981658935554, G_loss=44.64367423266172\n","Step=6499 D_loss=0.03198871254920954, G_loss=44.40393926382065\n","Step=6599 D_loss=0.03287687778472903, G_loss=43.40109263300896\n","Step=6699 D_loss=0.03698036313056963, G_loss=43.037885445356366\n","Step=6799 D_loss=0.042473833560943586, G_loss=43.43516255795956\n","Step=6899 D_loss=0.021070474386215343, G_loss=42.06428786486387\n","Step=6999 D_loss=0.007461401224136299, G_loss=41.136047076880935\n","Step=7099 D_loss=0.01808242797851567, G_loss=40.01523994266987\n","Step=7199 D_loss=0.021660873889923105, G_loss=45.295168240964415\n","Step=7299 D_loss=0.012830370664596469, G_loss=46.7522466096282\n","Step=7399 D_loss=0.006525321006775053, G_loss=43.28901439666748\n","Step=7499 D_loss=0.010575337409973073, G_loss=42.60592228412629\n","Step=7599 D_loss=0.012378329038619906, G_loss=43.93255503892899\n","Step=7699 D_loss=0.00800884604454044, G_loss=43.983769261837004\n","Step=7799 D_loss=0.01208141803741447, G_loss=47.250605026483534\n","Step=7899 D_loss=0.008253697156906048, G_loss=51.52163055300713\n","Step=7999 D_loss=0.008462946414947492, G_loss=48.61866044580937\n","Step=8099 D_loss=0.013771128654480158, G_loss=49.16177591830492\n","Step=8199 D_loss=0.008782926797866741, G_loss=45.89892210543156\n","Step=8299 D_loss=0.008574900627136284, G_loss=43.88777612864971\n","Step=8399 D_loss=0.009274421930313137, G_loss=49.09778828561306\n","Step=8499 D_loss=0.011764515638351458, G_loss=52.396510397791864\n","Step=8599 D_loss=0.00630256175994881, G_loss=54.21544924706221\n","Step=8699 D_loss=0.003562982082366828, G_loss=48.40875377207995\n","Step=8799 D_loss=0.0043572723865508145, G_loss=45.24990890562534\n","Step=8899 D_loss=0.003734977245330784, G_loss=44.63574389696121\n","Step=8999 D_loss=0.0015542352199553466, G_loss=43.56640481770039\n","Step=9099 D_loss=0.001098279953002823, G_loss=43.078623244464396\n","Step=9199 D_loss=0.004765666723251405, G_loss=42.24531163752079\n","Step=9299 D_loss=0.0028102171421049427, G_loss=44.56531418412924\n","Step=9399 D_loss=0.004480699300766, G_loss=43.04467567652463\n","Step=9499 D_loss=0.009325923919677681, G_loss=39.109933805763724\n","Step=9599 D_loss=0.008961871862411597, G_loss=36.74230711758136\n","Step=9699 D_loss=0.013828057050704778, G_loss=35.81922553658486\n","Step=9799 D_loss=0.01806171417236313, G_loss=32.929178195297716\n","Step=9899 D_loss=0.09882854938507091, G_loss=25.623240651488306\n","Step=9999 D_loss=1.447861946821213, G_loss=12.595195004940033\n","Step=10099 D_loss=1.02433211684227, G_loss=6.946630028784275\n","Step=10199 D_loss=1.6573904633522036, G_loss=7.340961859226226\n","Step=10299 D_loss=1.781400053501129, G_loss=4.953102132678032\n","Step=10399 D_loss=1.1477409827709197, G_loss=6.376343839466571\n","Step=10499 D_loss=1.2763303673267363, G_loss=3.719165420830249\n","Step=10599 D_loss=1.139846739768982, G_loss=4.478139730393886\n","Step=10699 D_loss=0.8987790131568909, G_loss=5.2831599599123\n","Step=10799 D_loss=1.1773041903972623, G_loss=3.868032566010952\n","Step=10899 D_loss=1.0773937344551086, G_loss=3.1231334435939786\n","Step=10999 D_loss=1.0033522295951842, G_loss=5.167836709618569\n","Step=11099 D_loss=0.9537619805336, G_loss=4.802397628128529\n","Step=11199 D_loss=1.6991330254077912, G_loss=3.6772359973192215\n","Step=11299 D_loss=0.7515226495265961, G_loss=4.1845359146595\n","Step=11399 D_loss=1.1639837145805358, G_loss=3.7808374556899076\n","Step=11499 D_loss=1.166283634901047, G_loss=2.996338983476162\n","Step=11599 D_loss=0.8586661624908447, G_loss=3.319134754538536\n","Step=11699 D_loss=0.7384272933006286, G_loss=2.8411276975274085\n","Step=11799 D_loss=1.210253998041153, G_loss=2.6969867616891863\n","Step=11899 D_loss=1.011604679822922, G_loss=3.985530726015568\n","Step=11999 D_loss=0.897678608894348, G_loss=2.624064709544182\n","Step=12099 D_loss=1.3494951963424682, G_loss=2.338284578621387\n","Step=12199 D_loss=0.6820635163784026, G_loss=4.383781090080738\n","Step=12299 D_loss=1.295677602291107, G_loss=3.3909438931941986\n","Step=12399 D_loss=0.9595456993579863, G_loss=2.445207495689392\n","Step=12499 D_loss=0.7819239759445191, G_loss=3.9269637301564213\n","Step=12599 D_loss=0.8447834634780882, G_loss=3.2097443395853045\n","Step=12699 D_loss=1.2792874705791473, G_loss=2.748919416666031\n","Step=12799 D_loss=1.116033285856247, G_loss=4.674483613669873\n","Step=12899 D_loss=1.4033168709278105, G_loss=3.06485415995121\n","Step=12999 D_loss=0.6191759264469148, G_loss=3.03162743806839\n","Step=13099 D_loss=0.7556425213813782, G_loss=3.537514523267746\n","Step=13199 D_loss=1.3521984255313875, G_loss=2.4004261741042137\n","Step=13299 D_loss=0.8917729318141938, G_loss=3.3060366329550743\n","Step=13399 D_loss=0.6448328793048859, G_loss=2.885980416536331\n","Step=13499 D_loss=0.5524344182014467, G_loss=3.3492554852366445\n","Step=13599 D_loss=1.554593143463135, G_loss=2.662562618851662\n","Step=13699 D_loss=1.2187238180637359, G_loss=4.492711353003979\n","Step=13799 D_loss=1.369178091287613, G_loss=2.3559252139925957\n","Step=13899 D_loss=0.8023393058776855, G_loss=2.000683660805225\n","Step=13999 D_loss=0.5609617340564728, G_loss=3.0592753124237064\n","Step=14099 D_loss=0.5012257814407348, G_loss=6.075233531296253\n","Step=14199 D_loss=1.4459271371364593, G_loss=2.4952089640498163\n","Step=14299 D_loss=1.2499334573745726, G_loss=4.993814569413662\n","Step=14399 D_loss=0.9432023763656616, G_loss=4.043933710455894\n","Step=14499 D_loss=0.9535472309589386, G_loss=2.600686987042427\n","Step=14599 D_loss=0.9097291755676269, G_loss=2.106142448484898\n","Step=14699 D_loss=0.659088841676712, G_loss=2.6466408282518388\n","Step=14799 D_loss=1.156424095630646, G_loss=2.1443177115917207\n","Step=14899 D_loss=1.1735282158851623, G_loss=3.67237829297781\n","Step=14999 D_loss=1.050949295759201, G_loss=3.488414020240307\n","Step=15099 D_loss=1.0695696902275087, G_loss=1.9534938991069792\n","Step=15199 D_loss=1.165883350968361, G_loss=2.385190908312797\n","Step=15299 D_loss=1.1118265151977536, G_loss=1.8815276074409486\n","Step=15399 D_loss=0.7312404030561448, G_loss=1.739744140505791\n","Step=15499 D_loss=0.6024759197235108, G_loss=2.0475623220205303\n","Step=15599 D_loss=0.6284307724237443, G_loss=2.984910505414009\n","Step=15699 D_loss=1.010565360188484, G_loss=3.2276391381025316\n","Step=15799 D_loss=0.8990032321214676, G_loss=1.8370631411671638\n","Step=15899 D_loss=0.7266013139486313, G_loss=2.163328791260719\n","Step=15999 D_loss=1.451315492391586, G_loss=1.8725244018435478\n","Step=16099 D_loss=0.8923334699869155, G_loss=4.000374679267406\n","Step=16199 D_loss=0.9848472374677658, G_loss=2.395063048303127\n","Step=16299 D_loss=1.0610713303089143, G_loss=2.2749619457125663\n","Step=16399 D_loss=0.7980304580926896, G_loss=2.2804653424024584\n","Step=16499 D_loss=0.37317983746528616, G_loss=4.1377198103070265\n","Step=16599 D_loss=0.9945330804586411, G_loss=3.11025349855423\n","Step=16699 D_loss=1.2639913308620456, G_loss=2.541245267689228\n","Step=16799 D_loss=0.3453117281198502, G_loss=3.8429902312159534\n","Step=16899 D_loss=1.1811040937900543, G_loss=1.5939568322896958\n","Step=16999 D_loss=0.7287563896179199, G_loss=3.7147935932874683\n","Step=17099 D_loss=1.2109788697957993, G_loss=2.219004852473736\n","Step=17199 D_loss=0.8169689154624938, G_loss=2.574820739328861\n","Step=17299 D_loss=0.3967627489566803, G_loss=3.444135591685772\n","Step=17399 D_loss=1.296672974228859, G_loss=2.0493845534324646\n","Step=17499 D_loss=0.699547485113144, G_loss=2.9969785687327386\n","Step=17599 D_loss=0.810946574807167, G_loss=2.903224204778671\n","Step=17699 D_loss=1.3169164782762528, G_loss=1.6986961022019385\n","Step=17799 D_loss=0.4686730045080185, G_loss=3.535521657764912\n","Step=17899 D_loss=0.8635438430309295, G_loss=2.100149443149567\n","Step=17999 D_loss=0.9006490606069565, G_loss=1.6375084629654884\n","Step=18099 D_loss=0.6376344621181489, G_loss=2.632804127037525\n","Step=18199 D_loss=0.9530778139829635, G_loss=1.7062775641679766\n","Step=18299 D_loss=0.7644009333848952, G_loss=3.314749752879143\n","Step=18399 D_loss=1.1043067538738252, G_loss=1.8412620380520823\n","Step=18499 D_loss=0.39013328015804294, G_loss=3.4996322441101073\n","Step=18599 D_loss=0.9126651591062547, G_loss=2.0001679420471192\n","Step=18699 D_loss=1.3206312739849089, G_loss=5.5297731527686125\n","Step=18799 D_loss=1.0493874448537825, G_loss=3.0454358595609667\n","Step=18899 D_loss=0.3876718884706497, G_loss=4.318629370629787\n","Step=18999 D_loss=0.7617963963747025, G_loss=3.0367002788186075\n","Step=19099 D_loss=1.1515655696392058, G_loss=1.486132016479969\n","Step=19199 D_loss=0.7439779263734817, G_loss=2.9759932246804235\n","Step=19299 D_loss=0.9216936135292053, G_loss=1.9842927810549735\n","Step=19399 D_loss=0.9238611888885497, G_loss=2.2919884699583055\n","Step=19499 D_loss=0.42014663457870494, G_loss=3.5360827296972275\n","Step=19599 D_loss=0.8872025811672211, G_loss=2.0828739255666733\n","Step=19699 D_loss=0.8462444508075715, G_loss=1.6952477231621743\n","Step=19799 D_loss=0.8908713269233703, G_loss=2.043765454292297\n","Step=19899 D_loss=1.0276821666955946, G_loss=2.241087335050106\n","Step=19999 D_loss=0.5475224655866623, G_loss=3.505686559677124\n","Step=20099 D_loss=1.1994799542427064, G_loss=2.093051435649395\n","Step=20199 D_loss=1.040747299194336, G_loss=1.7796985328197479\n","Step=20299 D_loss=0.9358384597301485, G_loss=1.621230743229389\n","Step=20399 D_loss=0.8725505900382995, G_loss=2.873744553029537\n","Step=20499 D_loss=0.882921102643013, G_loss=4.993876551687717\n","Step=20599 D_loss=1.160838313102722, G_loss=1.8230319958925247\n","Step=20699 D_loss=0.8668079823255538, G_loss=2.0755628630518914\n","Step=20799 D_loss=0.5342798167467118, G_loss=3.1980258241295814\n","Step=20899 D_loss=0.7100852364301682, G_loss=4.814474904537201\n","Step=20999 D_loss=1.5909842467308046, G_loss=2.5979839891195295\n","Step=21099 D_loss=1.0127007442712783, G_loss=3.139381826221943\n","Step=21199 D_loss=0.7455328184366227, G_loss=1.9595114547014236\n","Step=21299 D_loss=1.1444702023267745, G_loss=3.0298121592402456\n","Step=21399 D_loss=0.4904756987094878, G_loss=3.046827738583088\n","Step=21499 D_loss=0.6953439605236054, G_loss=1.8118502408266066\n","Step=21599 D_loss=1.1600964546203612, G_loss=1.792898415625095\n","Step=21699 D_loss=0.5773988491296768, G_loss=3.28600775629282\n","Step=21799 D_loss=1.54296035528183, G_loss=2.569473400115967\n","Step=21899 D_loss=1.0165228390693664, G_loss=4.022658979296684\n","Step=21999 D_loss=1.217357891201973, G_loss=1.9198037400841714\n","Step=22099 D_loss=0.8418974673748018, G_loss=2.645003816485405\n","Step=22199 D_loss=1.2219914561510086, G_loss=3.2137589785456653\n","Step=22299 D_loss=0.5160825514793397, G_loss=5.351950061023235\n","Step=22399 D_loss=1.6543981498479843, G_loss=1.6415894597768783\n","Step=22499 D_loss=0.7964806663990021, G_loss=3.2249896094203\n","Step=22599 D_loss=0.7536478048563003, G_loss=2.4128606659173966\n","Step=22699 D_loss=1.0859336376190187, G_loss=2.7252013808488846\n","Step=22799 D_loss=0.5313934969902039, G_loss=3.386822665631771\n","Step=22899 D_loss=1.027637537717819, G_loss=2.467405495345593\n","Step=22999 D_loss=0.8100953286886216, G_loss=1.985087428689003\n","Step=23099 D_loss=0.8430952042341232, G_loss=2.677206742167473\n","Step=23199 D_loss=1.38594844520092, G_loss=1.9870974290370942\n","Step=23299 D_loss=0.5859858202934265, G_loss=3.4443160870671274\n","Step=23399 D_loss=0.7587413752079011, G_loss=2.180081160962582\n","Step=23499 D_loss=0.7979006242752075, G_loss=2.399722453057766\n","Step=23599 D_loss=0.5268216919898986, G_loss=3.2259308600425722\n","Step=23699 D_loss=1.0393192422389985, G_loss=2.3629089632630347\n","Step=23799 D_loss=0.9126211559772491, G_loss=3.1155585661530494\n","Step=23899 D_loss=0.40742671489715576, G_loss=5.678646286725997\n","Step=23999 D_loss=0.9875118684768677, G_loss=2.8616197562217716\n","Step=24099 D_loss=0.9972918808460236, G_loss=1.763275757431984\n","Step=24199 D_loss=0.6682625150680542, G_loss=2.0270827314257622\n","Step=24299 D_loss=1.0437027513980865, G_loss=2.0626287493109707\n","Step=24399 D_loss=0.7239202451705932, G_loss=3.73768854111433\n","Step=24499 D_loss=0.8927746230363847, G_loss=3.247086003422737\n","Step=24599 D_loss=0.8743103420734406, G_loss=2.0351607370376588\n","Step=24699 D_loss=1.1695489507913588, G_loss=1.989862816631794\n","Step=24799 D_loss=0.6180111747980117, G_loss=2.7947103595733642\n","Step=24899 D_loss=0.7510956454277039, G_loss=2.0882393413782117\n","Step=24999 D_loss=0.5582065087556839, G_loss=2.5955882188677784\n","Step=25099 D_loss=0.7022096174955367, G_loss=2.0120942884683606\n","Step=25199 D_loss=0.7695199799537658, G_loss=2.539948538839817\n","Step=25299 D_loss=0.3809183931350707, G_loss=3.8201517382264143\n","Step=25399 D_loss=0.882062001824379, G_loss=2.1732258293032647\n","Step=25499 D_loss=1.0069924205541612, G_loss=2.5777403482794763\n","Step=25599 D_loss=0.8325771963596343, G_loss=2.2200128027796744\n","Step=25699 D_loss=1.4153055500984193, G_loss=1.5477626916766165\n","Step=25799 D_loss=0.6095372033119202, G_loss=2.0807147735357283\n","Step=25899 D_loss=0.8474723654985427, G_loss=1.8472788599133492\n","Step=25999 D_loss=0.7002559804916383, G_loss=2.1200794702768326\n","Step=26099 D_loss=0.8285000455379485, G_loss=1.6858609691262245\n","Step=26199 D_loss=1.1084048199653627, G_loss=2.22782673895359\n","Step=26299 D_loss=0.7586050105094909, G_loss=2.4701239138841626\n","Step=26399 D_loss=0.8502360677719115, G_loss=2.682494025528431\n","Step=26499 D_loss=0.5840334713459016, G_loss=2.640675848126411\n","Step=26599 D_loss=1.0810888653993604, G_loss=1.72370525598526\n","Step=26699 D_loss=0.5478276050090789, G_loss=2.3848689624667165\n","Step=26799 D_loss=1.244423977136612, G_loss=2.600392997264862\n","Step=26899 D_loss=0.43423207938671127, G_loss=4.645741798579693\n","Step=26999 D_loss=0.9647274196147919, G_loss=1.905862798690796\n","Step=27099 D_loss=0.7930851697921754, G_loss=2.1546235918998717\n","Step=27199 D_loss=0.8828612017631532, G_loss=3.3037316077947616\n","Step=27299 D_loss=1.5256248575448992, G_loss=3.089018426835537\n","Step=27399 D_loss=0.5626470124721527, G_loss=2.684513761997223\n","Step=27499 D_loss=0.7594660419225692, G_loss=7.510810373723507\n","Step=27599 D_loss=0.6926430124044419, G_loss=3.5980667856335637\n","Step=27699 D_loss=0.8749870324134827, G_loss=3.091622824072838\n","Step=27799 D_loss=0.6234060424566269, G_loss=2.6075619643926617\n","Step=27899 D_loss=1.0733691030740737, G_loss=1.81223906069994\n","Step=27999 D_loss=0.8514960253238679, G_loss=3.776661471426487\n","Step=28099 D_loss=1.3229104155302047, G_loss=3.0001967951655386\n","Step=28199 D_loss=0.9181661641597749, G_loss=2.0979505372047424\n","Step=28299 D_loss=1.094201652407646, G_loss=1.3750389617681504\n","Step=28399 D_loss=0.7241982728242873, G_loss=1.9997534880042076\n","Step=28499 D_loss=0.899897427558899, G_loss=1.6354010063409805\n","Step=28599 D_loss=0.7942486464977264, G_loss=2.0576418226957323\n","Step=28699 D_loss=0.3776762992143632, G_loss=3.378564685881138\n","Step=28799 D_loss=0.923318829536438, G_loss=1.8822460681200026\n","Step=28899 D_loss=0.5376410615444184, G_loss=3.6202437213063243\n","Step=28999 D_loss=0.5927090674638749, G_loss=3.2400200223922733\n","Step=29099 D_loss=0.9694148010015486, G_loss=1.8424187156558036\n","Step=29199 D_loss=0.7077506810426712, G_loss=1.849549354314804\n","Step=29299 D_loss=0.9112713605165481, G_loss=2.0809026616811757\n","Step=29399 D_loss=1.1465063607692718, G_loss=4.216338100433349\n","Step=29499 D_loss=0.6177220690250397, G_loss=4.340881175398827\n","Step=29599 D_loss=1.1259837102890016, G_loss=1.7778189948201177\n","Step=29699 D_loss=0.4713627386093141, G_loss=3.406130754053593\n","Step=29799 D_loss=0.89004645049572, G_loss=2.6034279936552047\n","Step=29899 D_loss=0.8363989579677582, G_loss=1.8644975319504735\n","Step=29999 D_loss=0.9205432331562041, G_loss=1.7708521223068239\n","Step=30099 D_loss=0.503879487514496, G_loss=3.356752048432827\n","Step=30199 D_loss=1.0007982379198075, G_loss=2.6382514318823818\n","Step=30299 D_loss=0.8111086988449095, G_loss=1.9826624539494513\n","Step=30399 D_loss=0.4686922454833984, G_loss=4.034609320759773\n","Step=30499 D_loss=0.8233433908224107, G_loss=2.498499449789524\n","Step=30599 D_loss=1.0432759708166124, G_loss=2.0478583917021753\n","Step=30699 D_loss=0.8837556457519532, G_loss=1.9217480638623234\n","Step=30799 D_loss=1.183609078526497, G_loss=1.856611005663872\n","Step=30899 D_loss=0.6045124846696854, G_loss=2.423739832341671\n","Step=30999 D_loss=0.6586579072475433, G_loss=2.202180037200451\n","Step=31099 D_loss=0.7430313205718994, G_loss=1.8304027900099753\n","Step=31199 D_loss=1.1684106296300887, G_loss=1.5535054984688756\n","Step=31299 D_loss=0.7023948848247527, G_loss=2.539287846684456\n","Step=31399 D_loss=0.38001573503017416, G_loss=2.65295035302639\n","Step=31499 D_loss=1.6065687918663025, G_loss=3.0211475223302844\n","Step=31599 D_loss=1.3999043494462966, G_loss=1.3807689341902734\n","Step=31699 D_loss=0.47206097483634946, G_loss=2.7421280628442766\n","Step=31799 D_loss=0.8495886367559433, G_loss=1.7759143275022506\n","Step=31899 D_loss=0.6826502168178559, G_loss=3.343646317720413\n","Step=31999 D_loss=0.8010471528768538, G_loss=1.826614207625389\n","Step=32099 D_loss=0.9653854858875275, G_loss=2.806352619826794\n","Step=32199 D_loss=0.865936371088028, G_loss=2.09999280244112\n","Step=32299 D_loss=0.8025466543436052, G_loss=1.6551596620678901\n","Step=32399 D_loss=0.7468421125411988, G_loss=1.8002290591597558\n","Step=32499 D_loss=0.7408535706996918, G_loss=1.9476403656601906\n","Step=32599 D_loss=0.9573913007974625, G_loss=1.957708777189255\n","Step=32699 D_loss=0.5954430842399598, G_loss=2.310952321887016\n","Step=32799 D_loss=1.0644967150688172, G_loss=1.8616062277555467\n","Step=32899 D_loss=0.837904800772667, G_loss=1.5888409325480464\n","Step=32999 D_loss=0.9379945611953735, G_loss=2.0054773285984995\n","Step=33099 D_loss=0.7816237097978591, G_loss=2.5422831064462663\n","Step=33199 D_loss=0.867094602584839, G_loss=1.7898534995317457\n","Step=33299 D_loss=0.7406890869140625, G_loss=2.2818749845027924\n","Step=33399 D_loss=0.4519014012813567, G_loss=3.4898640441894533\n","Step=33499 D_loss=1.2478619778156281, G_loss=1.7536810436844825\n","Step=33599 D_loss=0.5888746505975723, G_loss=3.0041323170065883\n","Step=33699 D_loss=1.394601370692253, G_loss=1.5755449873209\n","Step=33799 D_loss=0.6444418805837631, G_loss=1.8576996463537219\n","Step=33899 D_loss=0.5232658439874648, G_loss=2.9366800025105477\n","Step=33999 D_loss=1.1025238275527953, G_loss=1.6630623173713683\n","Step=34099 D_loss=0.8987695741653442, G_loss=2.7119575417041775\n","Step=34199 D_loss=0.3880284684896468, G_loss=5.910116478502751\n","Step=34299 D_loss=1.0397155517339707, G_loss=2.2711528852581977\n","Step=34399 D_loss=1.2823670822381974, G_loss=1.3957447269558907\n","Step=34499 D_loss=0.8690766680240631, G_loss=2.8485854741930963\n","Step=34599 D_loss=0.5667862236499788, G_loss=4.038080740272998\n","Step=34699 D_loss=0.44550701260566716, G_loss=2.8782395800948146\n","Step=34799 D_loss=1.0715379202365876, G_loss=2.396028043627739\n","Step=34899 D_loss=0.7949518787860871, G_loss=3.26740814357996\n","Step=34999 D_loss=0.9210229790210724, G_loss=1.3720241701602935\n","Step=35099 D_loss=0.7631116783618926, G_loss=1.85854316085577\n","Step=35199 D_loss=0.944221408367157, G_loss=2.067490730881691\n","Step=35299 D_loss=0.6102325683832168, G_loss=2.0053480300307274\n","Step=35399 D_loss=0.6404492336511612, G_loss=2.5342845422029496\n","Step=35499 D_loss=0.8220652538537979, G_loss=1.6508596608042718\n","Step=35599 D_loss=0.9529757475852966, G_loss=2.0050195613503456\n","Step=35699 D_loss=0.8551799839735033, G_loss=3.272802964448929\n","Step=35799 D_loss=0.8950515705347061, G_loss=2.2451058283448218\n","Step=35899 D_loss=0.9079967713356017, G_loss=1.672809417247772\n","Step=35999 D_loss=0.5190512627363205, G_loss=4.195546286702156\n","Step=36099 D_loss=0.9474138373136521, G_loss=1.893440447151661\n","Step=36199 D_loss=0.6256724554300309, G_loss=2.121835297048092\n","Step=36299 D_loss=1.391385564804077, G_loss=3.0286547955870624\n","Step=36399 D_loss=1.3154907262325288, G_loss=1.3307715380191802\n","Step=36499 D_loss=0.7435062444210053, G_loss=2.008673560321331\n","Step=36599 D_loss=1.0886242067813874, G_loss=1.8293560469150545\n","Step=36699 D_loss=0.6862323641777038, G_loss=2.9815423676371573\n","Step=36799 D_loss=1.136635656952858, G_loss=1.5083801898360252\n","Step=36899 D_loss=0.9097394251823425, G_loss=5.148276385962964\n","Step=36999 D_loss=0.5379199498891831, G_loss=4.306031534671783\n","Step=37099 D_loss=1.3077066618204118, G_loss=1.1275078403949736\n","Step=37199 D_loss=0.5835131436586379, G_loss=2.2030486536026\n","Step=37299 D_loss=0.6903756821155548, G_loss=1.7544599169492723\n","Step=37399 D_loss=1.0680855298042298, G_loss=2.359706650078297\n","Step=37499 D_loss=1.1733789157867434, G_loss=1.8712491175532342\n","Step=37599 D_loss=1.0345841211080553, G_loss=1.7889640602469443\n","Step=37699 D_loss=0.5277892047166824, G_loss=2.972944647669792\n","Step=37799 D_loss=1.0240041679143905, G_loss=1.7267532080411911\n","Step=37899 D_loss=0.661439219713211, G_loss=3.1970891398191448\n","Step=37999 D_loss=0.8894666081666946, G_loss=1.923558560013771\n","Step=38099 D_loss=0.9311637896299362, G_loss=1.2276290598511697\n","Step=38199 D_loss=0.6787728738784791, G_loss=1.6049504598975182\n","Step=38299 D_loss=0.7750211304426193, G_loss=2.0671455469727515\n","Step=38399 D_loss=0.920440189242363, G_loss=2.700394271016121\n","Step=38499 D_loss=1.354642826318741, G_loss=7.317870623469353\n","Step=38599 D_loss=1.1479063642024996, G_loss=1.2500884675979616\n","Step=38699 D_loss=0.9828006321191787, G_loss=1.879314795732498\n","Step=38799 D_loss=0.8312496852874756, G_loss=2.0178927755355835\n","Step=38899 D_loss=0.8381035417318344, G_loss=1.5592179346084594\n","Step=38999 D_loss=0.9637859851121903, G_loss=1.370955661535263\n","Step=39099 D_loss=0.8918775647878646, G_loss=1.5396162125468253\n","Step=39199 D_loss=0.5626425123214722, G_loss=2.896801329553127\n","Step=39299 D_loss=0.9220225703716277, G_loss=2.3035963779687885\n","Step=39399 D_loss=1.4803142046928408, G_loss=1.3643719574809074\n","Step=39499 D_loss=0.7498020243644715, G_loss=2.0303020119667052\n","Step=39599 D_loss=0.7003918826580048, G_loss=2.1745024341344834\n","Step=39699 D_loss=1.051325236558914, G_loss=1.4817135018110277\n","Step=39799 D_loss=1.0807238143682478, G_loss=1.5522177839279176\n","Step=39899 D_loss=0.7088634443283082, G_loss=1.6060668739676476\n","Step=39999 D_loss=0.8998027288913726, G_loss=1.518993361890316\n","Step=40099 D_loss=0.9249263232946395, G_loss=1.5232799667119978\n","Step=40199 D_loss=1.3565965151786805, G_loss=3.939971468150616\n","Step=40299 D_loss=0.6594830334186554, G_loss=2.8212268623709678\n","Step=40399 D_loss=0.9998831170797349, G_loss=1.7657824185490607\n","Step=40499 D_loss=1.38065957903862, G_loss=1.6475155168771742\n","Step=40599 D_loss=0.7470533949136734, G_loss=1.6048245373368266\n","Step=40699 D_loss=0.8791766875982285, G_loss=1.405241591334343\n","Step=40799 D_loss=1.0826434594392778, G_loss=1.5551011058688164\n","Step=40899 D_loss=0.7169044041633607, G_loss=2.833483005464077\n","Step=40999 D_loss=0.8417096310853958, G_loss=2.1315569838881494\n","Step=41099 D_loss=1.1931220960617066, G_loss=1.532382545173168\n","Step=41199 D_loss=0.6637364888191223, G_loss=2.016856456696987\n","Step=41299 D_loss=1.3355217397212982, G_loss=1.5200169613957406\n","Step=41399 D_loss=0.7370461875200272, G_loss=1.8480658879876135\n","Step=41499 D_loss=0.859098687171936, G_loss=1.4212624433636665\n","Step=41599 D_loss=0.7660985720157625, G_loss=2.153628700077534\n","Step=41699 D_loss=0.8929385513067245, G_loss=1.8695997345447544\n","Step=41799 D_loss=0.8774834907054901, G_loss=1.6466528311371804\n","Step=41899 D_loss=0.6335124838352203, G_loss=1.7050887981057168\n","Step=41999 D_loss=0.8865718746185303, G_loss=1.8621007388830184\n","Step=42099 D_loss=1.1727977502346039, G_loss=3.379814762175083\n","Step=42199 D_loss=1.0079881578683851, G_loss=1.7631673553586007\n","Step=42299 D_loss=0.7988896560668945, G_loss=3.625044200122356\n","Step=42399 D_loss=0.6722902333736419, G_loss=2.955076613724232\n","Step=42499 D_loss=0.6941934829950333, G_loss=1.5451940888166429\n","Step=42599 D_loss=0.9602824997901915, G_loss=2.2623382583260536\n","Step=42699 D_loss=1.1616988414525986, G_loss=1.6157553803920748\n","Step=42799 D_loss=0.9425289297103883, G_loss=1.3702687141299248\n","Step=42899 D_loss=0.5870831888914108, G_loss=1.73714512526989\n","Step=42999 D_loss=1.097004593014717, G_loss=1.0799506226181985\n","Step=43099 D_loss=0.6182336848974228, G_loss=2.461503641307354\n","Step=43199 D_loss=0.9501578336954117, G_loss=1.5843564620614052\n","Step=43299 D_loss=1.0941698044538497, G_loss=1.2864755299687385\n","Step=43399 D_loss=0.6183808583021164, G_loss=2.1754648008942605\n","Step=43499 D_loss=1.0005678790807724, G_loss=1.2572859281301498\n","Step=43599 D_loss=1.3589459705352782, G_loss=1.0903847911953926\n","Step=43699 D_loss=0.8076872545480729, G_loss=2.7064054808020592\n","Step=43799 D_loss=1.1523881381750107, G_loss=1.5516014119982717\n","Step=43899 D_loss=1.9162013262510298, G_loss=5.258152469992638\n","Step=43999 D_loss=0.9552778661251068, G_loss=2.1196559467911724\n","Step=44099 D_loss=1.0575661778450014, G_loss=1.0479953005909919\n","Step=44199 D_loss=1.0585740196704865, G_loss=1.4047148111462593\n","Step=44299 D_loss=0.8282411891221999, G_loss=1.4350369489192962\n","Step=44399 D_loss=0.8600685662031174, G_loss=1.2765959599614145\n","Step=44499 D_loss=1.4634433758258818, G_loss=2.979281778037548\n","Step=44599 D_loss=0.7761554312705994, G_loss=1.6816325968503953\n","Step=44699 D_loss=0.8330507063865661, G_loss=1.6276923108100891\n","Step=44799 D_loss=1.0174647879600525, G_loss=1.4752691325545313\n","Step=44899 D_loss=0.7014978104829789, G_loss=1.3000680992007254\n","Step=44999 D_loss=0.8052494198083877, G_loss=2.0289063239097596\n","Step=45099 D_loss=1.6109486603736878, G_loss=2.2865578287839887\n","Step=45199 D_loss=0.8362345296144484, G_loss=1.1435689276456833\n","Step=45299 D_loss=0.9719802752137183, G_loss=1.517138838469982\n","Step=45399 D_loss=0.7253641137480735, G_loss=2.9059402456879617\n","Step=45499 D_loss=0.8821476504206658, G_loss=1.4605015531182288\n","Step=45599 D_loss=0.5589690327644348, G_loss=2.5444139105081556\n","Step=45699 D_loss=1.4067840293049814, G_loss=1.3376102262735368\n","Step=45799 D_loss=0.9092396929860115, G_loss=1.8378060913085936\n","Step=45899 D_loss=0.987484627366066, G_loss=1.4593579295277594\n","Step=45999 D_loss=0.9236271131038667, G_loss=1.5007692596316338\n","Step=46099 D_loss=1.4743466544151307, G_loss=2.2452178540825845\n","Step=46199 D_loss=1.4931795805692671, G_loss=0.7889161312580109\n","Step=46299 D_loss=1.0524403679370882, G_loss=0.9762915524840354\n","Step=46399 D_loss=0.849073027074337, G_loss=1.6008482113480567\n","Step=46499 D_loss=0.8062033638358117, G_loss=1.7053630200028418\n","Step=46599 D_loss=0.9030260625481605, G_loss=1.6335009065270423\n","Step=46699 D_loss=0.8736195072531701, G_loss=2.0747401717305185\n","Step=46799 D_loss=0.7368998011946677, G_loss=1.687048543393612\n","Step=46899 D_loss=0.9395451113581659, G_loss=1.0883333545923235\n","Step=46999 D_loss=0.5483834746479987, G_loss=1.7383510529994963\n","Step=47099 D_loss=0.8933638340234757, G_loss=3.2983869171142577\n","Step=47199 D_loss=1.919104672074318, G_loss=1.5558102545142174\n","Step=47299 D_loss=0.9478437930345536, G_loss=1.6140192425251008\n","Step=47399 D_loss=0.898216364979744, G_loss=1.971020322740078\n","Step=47499 D_loss=0.4813658353686333, G_loss=2.335261012911797\n","Step=47599 D_loss=0.9881777220964432, G_loss=1.4490417140722274\n","Step=47699 D_loss=1.1782034415006637, G_loss=1.306725023984909\n","Step=47799 D_loss=1.0634961599111559, G_loss=1.1474163541197775\n","Step=47899 D_loss=1.2901729223132135, G_loss=1.2035153689980507\n","Step=47999 D_loss=1.0570838588476181, G_loss=1.3968110904097557\n","Step=48099 D_loss=0.9687645667791367, G_loss=1.2235608395934106\n","Step=48199 D_loss=0.779976105093956, G_loss=1.6084793478250503\n","Step=48299 D_loss=0.9612206107378005, G_loss=1.2603653529286385\n","Step=48399 D_loss=1.5021901586651802, G_loss=2.4253504449129104\n","Step=48499 D_loss=1.174932827949524, G_loss=1.016279438138008\n","Step=48599 D_loss=1.0256322699785232, G_loss=1.1479497036337853\n","Step=48699 D_loss=0.9469263255596162, G_loss=1.2339012190699576\n","Step=48799 D_loss=1.5904681611061098, G_loss=1.0846788439154627\n","Step=48899 D_loss=1.347983680665493, G_loss=1.7687338438630102\n","Step=48999 D_loss=0.9710602775216104, G_loss=1.0133568593859672\n","Step=49099 D_loss=1.0287172514200211, G_loss=1.3095435085892677\n","Step=49199 D_loss=0.8562373587489127, G_loss=1.2475250294804572\n","Step=49299 D_loss=0.873041160106659, G_loss=0.9973904693126678\n","Step=49399 D_loss=5.3906693035364155, G_loss=10.92814883083105\n","Step=49499 D_loss=2.400611927509308, G_loss=12.816297059059142\n","Step=49599 D_loss=1.1720285904407501, G_loss=3.6994284877181056\n","Step=49699 D_loss=1.6457971832156184, G_loss=0.9056776073575019\n","Step=49799 D_loss=1.3199251320958136, G_loss=0.8142015054821967\n","Step=49899 D_loss=0.9361793640255929, G_loss=1.104958592057228\n","Step=49999 D_loss=0.8826708769798278, G_loss=1.474115096628666\n","Reached 50001 epochs for GAN\n"],"name":"stdout"},{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAXoAAAEICAYAAABRSj9aAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nEr2ISMAp0YuIBJwSvYhIwCnRi4gE3P8HMRmhIyn7TvoAAAAASUVORK5CYII=\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[]}}]},{"cell_type":"code","metadata":{"id":"tFu0U0AlRMP1","colab_type":"code","colab":{}},"source":["#modeling CNN\n","\n","import tensorflow as tf\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import os\n","\n","SEED = 42\n","tf.set_random_seed(SEED)\n","class CNN():\n","\n","    def __init__(self, num_features, num_historical_days, is_train=True):\n","      \n","        self.X = tf.placeholder(tf.float32, shape=[None, num_historical_days, num_features])\n","        X = tf.reshape(self.X, [-1, num_historical_days, 1, num_features])\n","        self.Y = tf.placeholder(tf.int32, shape=[None, 2])\n","        self.keep_prob = tf.placeholder(tf.float32, shape=[])\n","\n","        with tf.variable_scope(\"cnn\"):\n","            #[filter_height, filter_width, in_channels, out_channels]\n","            k1 = tf.Variable(tf.truncated_normal([3, 1, num_features, 16],\n","                stddev=0.1,seed=SEED, dtype=tf.float32))\n","            b1 = tf.Variable(tf.zeros([16], dtype=tf.float32))\n","\n","            conv = tf.nn.conv2d(X,k1,strides=[1, 1, 1, 1],padding='SAME')\n","            relu = tf.nn.relu(tf.nn.bias_add(conv, b1))\n","            if is_train:\n","                relu = tf.nn.dropout(relu, keep_prob = self.keep_prob)\n","            print(relu)\n","\n","\n","            k2 = tf.Variable(tf.truncated_normal([3, 1, 16, 32],\n","                stddev=0.1,seed=SEED, dtype=tf.float32))\n","            b2 = tf.Variable(tf.zeros([32], dtype=tf.float32))\n","            conv = tf.nn.conv2d(relu, k2,strides=[1, 1, 1, 1],padding='SAME')\n","            relu = tf.nn.relu(tf.nn.bias_add(conv, b2))\n","            if is_train:\n","                relu = tf.nn.dropout(relu, keep_prob = self.keep_prob)\n","            print(relu)\n","\n","\n","            k3 = tf.Variable(tf.truncated_normal([3, 1, 32, 64],\n","                stddev=0.1,seed=SEED, dtype=tf.float32))\n","            b3 = tf.Variable(tf.zeros([64], dtype=tf.float32))\n","            conv = tf.nn.conv2d(relu, k3, strides=[1, 1, 1, 1], padding='VALID')\n","            relu = tf.nn.relu(tf.nn.bias_add(conv, b3))\n","            if is_train:\n","                relu = tf.nn.dropout(relu, keep_prob=self.keep_prob)\n","            print(relu)\n","\n","\n","            flattened_convolution_size = int(relu.shape[1]) * int(relu.shape[2]) * int(relu.shape[3])\n","            print(flattened_convolution_size)\n","            flattened_convolution = features = tf.reshape(relu, [-1, flattened_convolution_size])\n","\n","            if is_train:\n","                flattened_convolution =  tf.nn.dropout(flattened_convolution, keep_prob=self.keep_prob)\n","\n","            W1 = tf.Variable(tf.truncated_normal([18*1*64, 32]))\n","            b4 = tf.Variable(tf.truncated_normal([32]))\n","            h1 = tf.nn.relu(tf.matmul(flattened_convolution, W1) + b4)\n","\n","\n","            W2 = tf.Variable(tf.truncated_normal([32, 2]))\n","            logits = tf.matmul(h1, W2)\n","\n","            #self.accuracy = tf.metrics.accuracy(tf.argmax(self.Y, 1), tf.argmax(logits, 1))\n","            self.accuracy = tf.reduce_mean(tf.cast(tf.equal(tf.argmax(self.Y, 1), tf.argmax(logits, 1)), tf.float32))\n","            self.confusion_matrix = tf.confusion_matrix(tf.argmax(self.Y, 1), tf.argmax(logits, 1))\n","            tf.summary.scalar('accuracy', self.accuracy)\n","            theta_D = [k1, b1, k2, b2, k3, b3, W1, b4, W2]           \n","            \n","            # D_prob = tf.nn.sigmoid(D_logit)\n","\n","        self.loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=self.Y, logits=logits))\n","        tf.summary.scalar('loss', self.loss)\n","        # self.D_l2_loss = (0.0001 * tf.add_n([tf.nn.l2_loss(t) for t in theta_D]) / len(theta_D))\n","        # self.D_loss = D_loss_real + D_loss_fake + self.D_l2_loss\n","        # self.G_l2_loss = (0.00001 * tf.add_n([tf.nn.l2_loss(t) for t in theta_G]) / len(theta_G))\n","        # self.G_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=D_logit_fake, labels=tf.ones_like(D_logit_fake))) + self.G_l2_loss\n","\n","        self.optimizer = tf.train.AdamOptimizer(learning_rate=0.0001).minimize(self.loss)\n","        self.summary = tf.summary.merge_all()\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"SSW0oCUNTZ6Z","colab_type":"code","outputId":"3936455d-2e0f-4ff4-8074-a80204cd1ec3","executionInfo":{"status":"ok","timestamp":1564091471870,"user_tz":240,"elapsed":168977,"user":{"displayName":"nupur deshpande","photoUrl":"https://lh4.googleusercontent.com/-AQjf2oEW7kE/AAAAAAAAAAI/AAAAAAAAA1Y/ZFcRZICzWa0/s64/photo.jpg","userId":"04753088851981756999"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["#training CNN\n","\n","import os\n","import pandas as pd\n","# from cnn import CNN\n","import random\n","import tensorflow as tf\n","import xgboost as xgb\n","from sklearn.externals import joblib\n","from sklearn.metrics import confusion_matrix\n","# from plot_confusion_matrix import plot_confusion_matrix\n","\n","random.seed(42)\n","\n","class TrainCNN:\n","\n","    def __init__(self, num_historical_days, days=10, pct_change=0):\n","        self.data = []\n","        self.labels = []\n","        self.test_data = []\n","        self.test_labels = []\n","        self.cnn = CNN(num_features=5, num_historical_days=num_historical_days, is_train=False)\n","#         files = [os.path.join('./stock_data', f) for f in os.listdir('./stock_data')]\n","\n","        # Google Drive Method\n","        files = [f\"{googlepath}stock_data/{f}\" for f in os.listdir(f\"{googlepath}stock_data\")]\n","#         print(files)\n","    \n","    \n","        for file in files:\n","            print(file)\n","            df = pd.read_csv(file, index_col='timestamp', parse_dates=True)\n","            df = df[['open','high','low','close','volume']]\n","            # data for new column labels that will use the pct_change of the closing data.\n","            # pct_change measure change between current and prior element. Map these into a 1x2\n","            # array to show if the pct_change > (our desired threshold) or less than.\n","            labels = df.close.pct_change(days).map(lambda x: [int(x > pct_change/100.0), int(x <= pct_change/100.0)])\n","            \n","            # rolling normalization. (df - df.mean) / (df.max - df.min)\n","            df = ((df -\n","            df.rolling(num_historical_days).mean().shift(-num_historical_days))\n","            /(df.rolling(num_historical_days).max().shift(-num_historical_days)\n","            -df.rolling(num_historical_days).min().shift(-num_historical_days)))\n","            df['labels'] = labels\n","\n","            # doing pct_change will give some rows (like first row) a NaN value. Drop that.\n","            df = df.dropna()\n","\n","            # Do the testing data split\n","            test_df = df[:365]\n","            df = df[400:]\n","\n","            # get the predictors of the dataframe\n","            data = df[['open','high','low','close','volume']].values\n","\n","            # the response value\n","            labels = df['labels'].values\n","\n","            # start at num_historical_days and iterate the full length of the training\n","            # data at intervals of num_historical_days\n","            for i in range(num_historical_days, len(df), num_historical_days):\n","                # split the df into arrays of length num_historical_days and append\n","                # to data, i.e. array of df[curr - num_days : curr] -> a batch of values\n","                self.data.append(data[i-num_historical_days:i])\n","\n","                # appending if price went up or down in curr day of \"i\" we are looking\n","                # at\n","                self.labels.append(labels[i-1])\n","            \n","            # do same for test data\n","            data = test_df[['open','high','low','close','volume']].values\n","            labels = test_df['labels'].values\n","            for i in range(num_historical_days, len(test_df), 1):\n","                self.test_data.append(data[i-num_historical_days:i])\n","                self.test_labels.append(labels[i-1])\n","\n","    # a function to get a random_batch of data.\n","    def random_batch(self, batch_size=128):\n","        batch = []\n","        labels = []\n","        # zip concatenates each array index of both arrays together\n","        data = list(zip(self.data, self.labels))\n","        i = 0\n","        while True:\n","            i+= 1\n","            while True:\n","                # pick a random array, i.e. range of days, from data\n","                d = random.choice(data)\n","                # balance the data with equal number of positive pct_change\n","                # and negative pct_change\n","                if(d[1][0]== int(i%2)):\n","                    break\n","            batch.append(d[0])  # append the range of days we got to batch\n","            labels.append(d[1])  # append the label of that range of data we got\n","            if (len(batch) == batch_size):\n","                yield batch, labels\n","                batch = []\n","                labels = []\n","\n","    def train(self, print_steps=100, display_steps=100, save_steps=SAVE_STEPS_AMOUNT, batch_size=128, keep_prob=0.6):\n","        if not os.path.exists(f'{googlepath}cnn_models'):\n","            os.makedirs(f'{googlepath}cnn_models')\n","        if not os.path.exists(f'{googlepath}logs'):\n","            os.makedirs(f'{googlepath}logs')\n","        if os.path.exists(f'{googlepath}logs/train'):\n","            for file in [os.path.join(f'{googlepath}logs/train/', f) for f in os.listdir(f'{googlepath}logs/train/')]:\n","                os.remove(file)\n","        if os.path.exists(f'{googlepath}logs/test'):\n","            for file in [os.path.join(f'{googlepath}logs/test/', f) for f in os.listdir(f'{googlepath}logs/test')]:\n","                os.remove(file)\n","\n","        sess = tf.Session()\n","        loss = 0\n","        l2_loss = 0\n","        accuracy = 0\n","        saver = tf.train.Saver()\n","        train_writer = tf.summary.FileWriter(f'{googlepath}/logs/train')\n","        test_writer = tf.summary.FileWriter(f'{googlepath}/logs/test')\n","        sess.run(tf.global_variables_initializer())\n","        \n","        test_loss_array = []\n","        test_accuracy_array = []\n","        currentStep = \"0\"\n","        \n","        if os.path.exists(f'{googlepath}cnn_models/checkpoint'):\n","                with open(f'{googlepath}cnn_models/checkpoint', 'rb') as f:\n","                    model_name = next(f).split('\"'.encode())[1]\n","                filename = \"{}cnn_models/{}\".format(googlepath, model_name.decode())\n","                currentStep = filename.split(\"-\")[1]\n","                new_saver = tf.train.import_meta_graph('{}.meta'.format(filename))\n","                new_saver.restore(sess, \"{}\".format(filename))\n","\n","        for i, [X, y] in enumerate(self.random_batch(batch_size)):\n","\n","          \n","            _, loss_curr, accuracy_curr = sess.run([self.cnn.optimizer, self.cnn.loss, self.cnn.accuracy], feed_dict=\n","                    {self.cnn.X:X, self.cnn.Y:y, self.cnn.keep_prob:keep_prob})\n","            loss += loss_curr\n","            accuracy += accuracy_curr\n","            if (i+1) % print_steps == 0:\n","                print('Step={} loss={}, accuracy={}'.format(i + int(currentStep), loss/print_steps, accuracy/print_steps))\n","                loss = 0\n","                l2_loss = 0\n","                accuracy = 0\n","                test_loss, test_accuracy, confusion_matrix = sess.run([self.cnn.loss, self.cnn.accuracy, self.cnn.confusion_matrix], feed_dict={self.cnn.X:self.test_data, self.cnn.Y:self.test_labels, self.cnn.keep_prob:1})\n","                test_loss_array.append(test_loss)\n","                test_accuracy_array.append(test_accuracy)\n","                print(\"Test loss = {}, Test accuracy = {}\".format(test_loss, test_accuracy))\n","            if (i+1) % save_steps == 0:\n","                saver.save(sess,  f'{googlepath}cnn_models/cnn.ckpt', i)\n","\n","            if (i+1) % display_steps == 0:\n","                summary = sess.run(self.cnn.summary, feed_dict=\n","                    {self.cnn.X:X, self.cnn.Y:y, self.cnn.keep_prob:keep_prob})\n","                train_writer.add_summary(summary, i)\n","                summary = sess.run(self.cnn.summary, feed_dict={\n","                    self.cnn.X:self.test_data, self.cnn.Y:self.test_labels, self.cnn.keep_prob:1})\n","                test_writer.add_summary(summary, i)\n","            \n","            # end training at training_amount epochs\n","            if (i + int(currentStep)) > TRAINING_AMOUNT:\n","                print(\"Reached {} epochs for CNN\".format(i + int(currentStep)))\n","                sess.close()\n","                print(confusion_matrix)\n","                plot_confusion_matrix(confusion_matrix, ['Down', 'Up'], normalize=True, title=\"CNN Confusion Matrix\")\n","                \n","                axisA = np.arange(0,len(test_loss_array),1)\n","                axisB = np.arange(0,len(test_accuracy_array),1)\n","                plt.plot(axisA, test_loss_array, label='test loss')\n","                plt.plot(axisB, test_accuracy_array, label='test accuracy')\n","                plt.legend()\n","                plt.title('test loss and accuracy')\n","                plt.show()\n","\n","                break\n","\n","\n","# if __name__ == '__main__':\n","tf.reset_default_graph()\n","cnn = TrainCNN(num_historical_days=HISTORICAL_DAYS_AMOUNT, days=DAYS_AHEAD, pct_change=PCT_CHANGE_AMOUNT)\n","cnn.train()\n","\n"],"execution_count":75,"outputs":[{"output_type":"stream","text":["Tensor(\"cnn/Relu:0\", shape=(?, 20, 1, 16), dtype=float32)\n","Tensor(\"cnn/Relu_1:0\", shape=(?, 20, 1, 32), dtype=float32)\n","Tensor(\"cnn/Relu_2:0\", shape=(?, 18, 1, 64), dtype=float32)\n","1152\n","/content/drive/My Drive/Colab Notebooks/SeniorDesignGAN/MiloGAN/stock_data/AMZN.csv\n","/content/drive/My Drive/Colab Notebooks/SeniorDesignGAN/MiloGAN/stock_data/AAPL.csv\n","Step=99 loss=0.6808989128470421, accuracy=0.674609375\n","Test loss = 0.5638794302940369, Test accuracy = 0.7217391133308411\n","Step=199 loss=0.40181615978479385, accuracy=0.829453125\n","Test loss = 0.49724796414375305, Test accuracy = 0.769565224647522\n","Step=299 loss=0.3034666983783245, accuracy=0.90078125\n","Test loss = 0.4646162688732147, Test accuracy = 0.7797101736068726\n","Step=399 loss=0.24045984357595443, accuracy=0.93921875\n","Test loss = 0.4684075713157654, Test accuracy = 0.7927536368370056\n","Step=499 loss=0.19585732519626617, accuracy=0.95234375\n","Test loss = 0.47597217559814453, Test accuracy = 0.7840579748153687\n","Step=599 loss=0.15732933893799783, accuracy=0.968984375\n","Test loss = 0.49504393339157104, Test accuracy = 0.7855072617530823\n","Step=699 loss=0.12831719666719438, accuracy=0.9746875\n","Test loss = 0.530319333076477, Test accuracy = 0.7855072617530823\n","Step=799 loss=0.10257050938904286, accuracy=0.98140625\n","Test loss = 0.5783989429473877, Test accuracy = 0.7797101736068726\n","Step=899 loss=0.0819173101335764, accuracy=0.988203125\n","Test loss = 0.5874494314193726, Test accuracy = 0.7797101736068726\n","Step=999 loss=0.0711246145516634, accuracy=0.991875\n","Test loss = 0.6330881118774414, Test accuracy = 0.7710145115852356\n","Step=1099 loss=0.057126974016427995, accuracy=0.994140625\n","Test loss = 0.6773044466972351, Test accuracy = 0.7681159377098083\n","Step=1199 loss=0.04523889185860753, accuracy=0.99546875\n","Test loss = 0.7083472013473511, Test accuracy = 0.7753623127937317\n","Step=1299 loss=0.03892286622896791, accuracy=0.9959375\n","Test loss = 0.749701976776123, Test accuracy = 0.7710145115852356\n","Step=1399 loss=0.031206135395914315, accuracy=0.997578125\n","Test loss = 0.7998055815696716, Test accuracy = 0.7710145115852356\n","Step=1499 loss=0.02614267368800938, accuracy=0.999140625\n","Test loss = 0.7996082901954651, Test accuracy = 0.7768115997314453\n","Step=1599 loss=0.021903015654534102, accuracy=1.0\n","Test loss = 0.8687381744384766, Test accuracy = 0.7652173638343811\n","Step=1699 loss=0.01875217253342271, accuracy=1.0\n","Test loss = 0.8833664059638977, Test accuracy = 0.7652173638343811\n","Step=1799 loss=0.015379560319706798, accuracy=1.0\n","Test loss = 0.9150763750076294, Test accuracy = 0.7652173638343811\n","Step=1899 loss=0.012673206445761025, accuracy=1.0\n","Test loss = 0.9276954531669617, Test accuracy = 0.7710145115852356\n","Step=1999 loss=0.011396597898565232, accuracy=1.0\n","Test loss = 0.9998152852058411, Test accuracy = 0.7652173638343811\n","Step=2099 loss=0.009311667215079068, accuracy=1.0\n","Test loss = 0.9683927893638611, Test accuracy = 0.7724637389183044\n","Step=2199 loss=0.008272986016236246, accuracy=1.0\n","Test loss = 1.011924386024475, Test accuracy = 0.769565224647522\n","Step=2299 loss=0.0073309137299656865, accuracy=1.0\n","Test loss = 1.029085636138916, Test accuracy = 0.7710145115852356\n","Step=2399 loss=0.006190845984965563, accuracy=1.0\n","Test loss = 1.0550572872161865, Test accuracy = 0.7681159377098083\n","Step=2499 loss=0.005513904995750636, accuracy=1.0\n","Test loss = 1.0679291486740112, Test accuracy = 0.769565224647522\n","Step=2599 loss=0.0048662914964370425, accuracy=1.0\n","Test loss = 1.1124036312103271, Test accuracy = 0.7652173638343811\n","Step=2699 loss=0.004357873157132417, accuracy=1.0\n","Test loss = 1.1218101978302002, Test accuracy = 0.7652173638343811\n","Step=2799 loss=0.00394484793767333, accuracy=1.0\n","Test loss = 1.1578826904296875, Test accuracy = 0.7652173638343811\n","Step=2899 loss=0.0035653680679388344, accuracy=1.0\n","Test loss = 1.1920711994171143, Test accuracy = 0.7666666507720947\n","Step=2999 loss=0.003228273713029921, accuracy=1.0\n","Test loss = 1.1906421184539795, Test accuracy = 0.7652173638343811\n","Step=3099 loss=0.0028096597804687915, accuracy=1.0\n","Test loss = 1.2090314626693726, Test accuracy = 0.7637681365013123\n","Step=3199 loss=0.0025938811514060944, accuracy=1.0\n","Test loss = 1.253064751625061, Test accuracy = 0.7623188495635986\n","Step=3299 loss=0.0022422392468433826, accuracy=1.0\n","Test loss = 1.2567201852798462, Test accuracy = 0.7594202756881714\n","Step=3399 loss=0.0020325737562961878, accuracy=1.0\n","Test loss = 1.2821485996246338, Test accuracy = 0.7594202756881714\n","Step=3499 loss=0.0018410682061221451, accuracy=1.0\n","Test loss = 1.2974121570587158, Test accuracy = 0.7579709887504578\n","Step=3599 loss=0.0016556429141201079, accuracy=1.0\n","Test loss = 1.3385428190231323, Test accuracy = 0.760869562625885\n","Step=3699 loss=0.001537855616188608, accuracy=1.0\n","Test loss = 1.3432382345199585, Test accuracy = 0.7579709887504578\n","Step=3799 loss=0.0014287876803427935, accuracy=1.0\n","Test loss = 1.3384478092193604, Test accuracy = 0.7637681365013123\n","Step=3899 loss=0.0012792275258107111, accuracy=1.0\n","Test loss = 1.3812408447265625, Test accuracy = 0.7579709887504578\n","Step=3999 loss=0.0011595264862990008, accuracy=1.0\n","Test loss = 1.3992613554000854, Test accuracy = 0.7565217614173889\n","Step=4099 loss=0.0010328382649458945, accuracy=1.0\n","Test loss = 1.4129225015640259, Test accuracy = 0.7565217614173889\n","Step=4199 loss=0.0009620826429454609, accuracy=1.0\n","Test loss = 1.4231603145599365, Test accuracy = 0.7565217614173889\n","Step=4299 loss=0.000916868404019624, accuracy=1.0\n","Test loss = 1.4526911973953247, Test accuracy = 0.7579709887504578\n","Step=4399 loss=0.0008142132422653958, accuracy=1.0\n","Test loss = 1.456567645072937, Test accuracy = 0.7594202756881714\n","Step=4499 loss=0.0007303203744231723, accuracy=1.0\n","Test loss = 1.4811718463897705, Test accuracy = 0.7565217614173889\n","Step=4599 loss=0.000727472169091925, accuracy=1.0\n","Test loss = 1.505175232887268, Test accuracy = 0.7550724744796753\n","Step=4699 loss=0.0006405252986587584, accuracy=1.0\n","Test loss = 1.4904723167419434, Test accuracy = 0.7594202756881714\n","Step=4799 loss=0.0006207596699823626, accuracy=1.0\n","Test loss = 1.5188597440719604, Test accuracy = 0.7579709887504578\n","Step=4899 loss=0.0005536690697772428, accuracy=1.0\n","Test loss = 1.5491241216659546, Test accuracy = 0.752173900604248\n","Step=4999 loss=0.000499446647008881, accuracy=1.0\n","Test loss = 1.5558257102966309, Test accuracy = 0.7550724744796753\n","Step=5099 loss=0.0004727859125705436, accuracy=1.0\n","Test loss = 1.5703644752502441, Test accuracy = 0.7550724744796753\n","Step=5199 loss=0.0004470916595892049, accuracy=1.0\n","Test loss = 1.6166660785675049, Test accuracy = 0.7507246136665344\n","Step=5299 loss=0.0004004426923347637, accuracy=1.0\n","Test loss = 1.6028485298156738, Test accuracy = 0.7550724744796753\n","Step=5399 loss=0.0003875269889249466, accuracy=1.0\n","Test loss = 1.6250746250152588, Test accuracy = 0.752173900604248\n","Step=5499 loss=0.0003523805517761502, accuracy=1.0\n","Test loss = 1.6375435590744019, Test accuracy = 0.752173900604248\n","Step=5599 loss=0.0003237060940591618, accuracy=1.0\n","Test loss = 1.6590354442596436, Test accuracy = 0.752173900604248\n","Step=5699 loss=0.00029391813179245217, accuracy=1.0\n","Test loss = 1.6577123403549194, Test accuracy = 0.7536231875419617\n","Step=5799 loss=0.00027709447269444356, accuracy=1.0\n","Test loss = 1.6927460432052612, Test accuracy = 0.752173900604248\n","Step=5899 loss=0.0002519638338708319, accuracy=1.0\n","Test loss = 1.677602767944336, Test accuracy = 0.752173900604248\n","Step=5999 loss=0.0002412576366623398, accuracy=1.0\n","Test loss = 1.7438271045684814, Test accuracy = 0.752173900604248\n","Step=6099 loss=0.00021976213160087354, accuracy=1.0\n","Test loss = 1.74461030960083, Test accuracy = 0.7507246136665344\n","Step=6199 loss=0.00021186140234931373, accuracy=1.0\n","Test loss = 1.7496141195297241, Test accuracy = 0.7507246136665344\n","Step=6299 loss=0.00020275861978007016, accuracy=1.0\n","Test loss = 1.7695119380950928, Test accuracy = 0.7492753863334656\n","Step=6399 loss=0.00018048436802928336, accuracy=1.0\n","Test loss = 1.7789216041564941, Test accuracy = 0.7492753863334656\n","Step=6499 loss=0.0001753380097215995, accuracy=1.0\n","Test loss = 1.7671176195144653, Test accuracy = 0.752173900604248\n","Step=6599 loss=0.00016297542373649776, accuracy=1.0\n","Test loss = 1.8073508739471436, Test accuracy = 0.7507246136665344\n","Step=6699 loss=0.00014810337088420056, accuracy=1.0\n","Test loss = 1.81759512424469, Test accuracy = 0.7492753863334656\n","Step=6799 loss=0.00013530323834856972, accuracy=1.0\n","Test loss = 1.8434056043624878, Test accuracy = 0.7492753863334656\n","Step=6899 loss=0.00012533217850432265, accuracy=1.0\n","Test loss = 1.8574901819229126, Test accuracy = 0.7492753863334656\n","Step=6999 loss=0.00011689935599861201, accuracy=1.0\n","Test loss = 1.8572187423706055, Test accuracy = 0.7492753863334656\n","Step=7099 loss=0.00011180258719832636, accuracy=1.0\n","Test loss = 1.8843157291412354, Test accuracy = 0.7507246136665344\n","Step=7199 loss=0.00010417840236186749, accuracy=1.0\n","Test loss = 1.8970434665679932, Test accuracy = 0.7507246136665344\n","Step=7299 loss=9.532159834634513e-05, accuracy=1.0\n","Test loss = 1.8954576253890991, Test accuracy = 0.7492753863334656\n","Step=7399 loss=8.585159175709123e-05, accuracy=1.0\n","Test loss = 1.9128597974777222, Test accuracy = 0.7492753863334656\n","Step=7499 loss=8.58254247214063e-05, accuracy=1.0\n","Test loss = 1.9325331449508667, Test accuracy = 0.7492753863334656\n","Step=7599 loss=7.48858432052657e-05, accuracy=1.0\n","Test loss = 1.9539251327514648, Test accuracy = 0.7507246136665344\n","Step=7699 loss=7.390393253444926e-05, accuracy=1.0\n","Test loss = 1.9533613920211792, Test accuracy = 0.7492753863334656\n","Step=7799 loss=6.845419033197686e-05, accuracy=1.0\n","Test loss = 1.9703372716903687, Test accuracy = 0.7492753863334656\n","Step=7899 loss=6.368087459122762e-05, accuracy=1.0\n","Test loss = 1.9919707775115967, Test accuracy = 0.7507246136665344\n","Step=7999 loss=5.897145387280034e-05, accuracy=1.0\n","Test loss = 2.0102274417877197, Test accuracy = 0.7507246136665344\n","Step=8099 loss=5.6320621115446554e-05, accuracy=1.0\n","Test loss = 2.020847797393799, Test accuracy = 0.7507246136665344\n","Step=8199 loss=5.1567836708272806e-05, accuracy=1.0\n","Test loss = 2.0417392253875732, Test accuracy = 0.7507246136665344\n","Step=8299 loss=4.678431987485965e-05, accuracy=1.0\n","Test loss = 2.0569701194763184, Test accuracy = 0.7507246136665344\n","Step=8399 loss=4.635529297956964e-05, accuracy=1.0\n","Test loss = 2.060917615890503, Test accuracy = 0.7507246136665344\n","Step=8499 loss=4.113006825718912e-05, accuracy=1.0\n","Test loss = 2.076820135116577, Test accuracy = 0.7507246136665344\n","Step=8599 loss=4.1563766899344046e-05, accuracy=1.0\n","Test loss = 2.091684341430664, Test accuracy = 0.7507246136665344\n","Step=8699 loss=3.706425814016256e-05, accuracy=1.0\n","Test loss = 2.080173969268799, Test accuracy = 0.7492753863334656\n","Step=8799 loss=3.5064270396105715e-05, accuracy=1.0\n","Test loss = 2.1122450828552246, Test accuracy = 0.752173900604248\n","Step=8899 loss=3.360254291692399e-05, accuracy=1.0\n","Test loss = 2.1277551651000977, Test accuracy = 0.752173900604248\n","Step=8999 loss=2.992107009049505e-05, accuracy=1.0\n","Test loss = 2.1498141288757324, Test accuracy = 0.752173900604248\n","Step=9099 loss=2.865933094653883e-05, accuracy=1.0\n","Test loss = 2.1664862632751465, Test accuracy = 0.752173900604248\n","Step=9199 loss=2.7192092129553203e-05, accuracy=1.0\n","Test loss = 2.1592650413513184, Test accuracy = 0.7507246136665344\n","Step=9299 loss=2.5372321315444423e-05, accuracy=1.0\n","Test loss = 2.1877665519714355, Test accuracy = 0.7507246136665344\n","Step=9399 loss=2.3977593118615915e-05, accuracy=1.0\n","Test loss = 2.1985044479370117, Test accuracy = 0.7507246136665344\n","Step=9499 loss=2.2804014406574424e-05, accuracy=1.0\n","Test loss = 2.2071118354797363, Test accuracy = 0.7507246136665344\n","Step=9599 loss=2.0541373478408785e-05, accuracy=1.0\n","Test loss = 2.2138471603393555, Test accuracy = 0.7507246136665344\n","Step=9699 loss=1.925998580190935e-05, accuracy=1.0\n","Test loss = 2.224174976348877, Test accuracy = 0.7507246136665344\n","Step=9799 loss=1.855633709965332e-05, accuracy=1.0\n","Test loss = 2.236241102218628, Test accuracy = 0.7507246136665344\n","Step=9899 loss=1.7506424892417272e-05, accuracy=1.0\n","Test loss = 2.2463669776916504, Test accuracy = 0.7507246136665344\n","Step=9999 loss=1.6166507375601213e-05, accuracy=1.0\n","Test loss = 2.2576491832733154, Test accuracy = 0.7507246136665344\n","Step=10099 loss=1.4901034692229587e-05, accuracy=1.0\n","Test loss = 2.2790091037750244, Test accuracy = 0.7507246136665344\n","Step=10199 loss=1.4522569235850824e-05, accuracy=1.0\n","Test loss = 2.2964026927948, Test accuracy = 0.7507246136665344\n","Step=10299 loss=1.3334689338080353e-05, accuracy=1.0\n","Test loss = 2.31421160697937, Test accuracy = 0.7507246136665344\n","Step=10399 loss=1.272588749998249e-05, accuracy=1.0\n","Test loss = 2.322469472885132, Test accuracy = 0.7507246136665344\n","Step=10499 loss=1.174540648207767e-05, accuracy=1.0\n","Test loss = 2.3260836601257324, Test accuracy = 0.752173900604248\n","Step=10599 loss=1.1346966689416149e-05, accuracy=1.0\n","Test loss = 2.3371682167053223, Test accuracy = 0.7536231875419617\n","Step=10699 loss=1.08562109835475e-05, accuracy=1.0\n","Test loss = 2.357273817062378, Test accuracy = 0.752173900604248\n","Step=10799 loss=1.0012084740083082e-05, accuracy=1.0\n","Test loss = 2.3653903007507324, Test accuracy = 0.7536231875419617\n","Step=10899 loss=9.294530555052916e-06, accuracy=1.0\n","Test loss = 2.3647215366363525, Test accuracy = 0.7550724744796753\n","Step=10999 loss=8.936991118844162e-06, accuracy=1.0\n","Test loss = 2.3853468894958496, Test accuracy = 0.7536231875419617\n","Step=11099 loss=8.28195196845627e-06, accuracy=1.0\n","Test loss = 2.4157252311706543, Test accuracy = 0.7536231875419617\n","Step=11199 loss=7.733259226370137e-06, accuracy=1.0\n","Test loss = 2.4162817001342773, Test accuracy = 0.7550724744796753\n","Step=11299 loss=7.3406078172411075e-06, accuracy=1.0\n","Test loss = 2.4164767265319824, Test accuracy = 0.7550724744796753\n","Step=11399 loss=6.782608934372547e-06, accuracy=1.0\n","Test loss = 2.4464995861053467, Test accuracy = 0.7550724744796753\n","Step=11499 loss=6.700319795527321e-06, accuracy=1.0\n","Test loss = 2.468625068664551, Test accuracy = 0.7536231875419617\n","Step=11599 loss=6.137413647593348e-06, accuracy=1.0\n","Test loss = 2.441882610321045, Test accuracy = 0.7550724744796753\n","Step=11699 loss=5.529036418465693e-06, accuracy=1.0\n","Test loss = 2.4671566486358643, Test accuracy = 0.7550724744796753\n","Step=11799 loss=5.367247865706304e-06, accuracy=1.0\n","Test loss = 2.507864475250244, Test accuracy = 0.7536231875419617\n","Step=11899 loss=5.114461550874694e-06, accuracy=1.0\n","Test loss = 2.527987241744995, Test accuracy = 0.752173900604248\n","Step=11999 loss=4.67933092295425e-06, accuracy=1.0\n","Test loss = 2.547316312789917, Test accuracy = 0.7550724744796753\n","Step=12099 loss=4.496031115195365e-06, accuracy=1.0\n","Test loss = 2.5330896377563477, Test accuracy = 0.7550724744796753\n","Step=12199 loss=4.222368945647759e-06, accuracy=1.0\n","Test loss = 2.5609662532806396, Test accuracy = 0.7536231875419617\n","Step=12299 loss=3.860335925764957e-06, accuracy=1.0\n","Test loss = 2.5650618076324463, Test accuracy = 0.7550724744796753\n","Step=12399 loss=3.7500967141568252e-06, accuracy=1.0\n","Test loss = 2.5777339935302734, Test accuracy = 0.7536231875419617\n","Step=12499 loss=3.479824492842454e-06, accuracy=1.0\n","Test loss = 2.5830764770507812, Test accuracy = 0.7550724744796753\n","Step=12599 loss=3.2671314215804157e-06, accuracy=1.0\n","Test loss = 2.604201078414917, Test accuracy = 0.7536231875419617\n","Step=12699 loss=3.048574178592389e-06, accuracy=1.0\n","Test loss = 2.5994176864624023, Test accuracy = 0.7550724744796753\n","Step=12799 loss=3.0264826591519522e-06, accuracy=1.0\n","Test loss = 2.6278810501098633, Test accuracy = 0.7550724744796753\n","Step=12899 loss=2.7379462892440642e-06, accuracy=1.0\n","Test loss = 2.6093509197235107, Test accuracy = 0.7536231875419617\n","Step=12999 loss=2.54408387490912e-06, accuracy=1.0\n","Test loss = 2.643411159515381, Test accuracy = 0.7550724744796753\n","Step=13099 loss=2.4483834499733346e-06, accuracy=1.0\n","Test loss = 2.6457293033599854, Test accuracy = 0.7536231875419617\n","Step=13199 loss=2.2970917075326724e-06, accuracy=1.0\n","Test loss = 2.653104305267334, Test accuracy = 0.7536231875419617\n","Step=13299 loss=2.1632345033140157e-06, accuracy=1.0\n","Test loss = 2.667961835861206, Test accuracy = 0.7536231875419617\n","Step=13399 loss=2.076771683050538e-06, accuracy=1.0\n","Test loss = 2.6777989864349365, Test accuracy = 0.7536231875419617\n","Step=13499 loss=1.9178989441570593e-06, accuracy=1.0\n","Test loss = 2.7170844078063965, Test accuracy = 0.752173900604248\n","Step=13599 loss=1.8102489264038012e-06, accuracy=1.0\n","Test loss = 2.722109794616699, Test accuracy = 0.752173900604248\n","Step=13699 loss=1.7364419250043283e-06, accuracy=1.0\n","Test loss = 2.720414638519287, Test accuracy = 0.752173900604248\n","Step=13799 loss=1.6484702950947395e-06, accuracy=1.0\n","Test loss = 2.741774797439575, Test accuracy = 0.752173900604248\n","Step=13899 loss=1.4929315477729687e-06, accuracy=1.0\n","Test loss = 2.745807647705078, Test accuracy = 0.752173900604248\n","Step=13999 loss=1.4528943171399077e-06, accuracy=1.0\n","Test loss = 2.7645208835601807, Test accuracy = 0.7507246136665344\n","Step=14099 loss=1.3561400152184433e-06, accuracy=1.0\n","Test loss = 2.7720367908477783, Test accuracy = 0.752173900604248\n","Step=14199 loss=1.2841771865623741e-06, accuracy=1.0\n","Test loss = 2.7811429500579834, Test accuracy = 0.7507246136665344\n","Step=14299 loss=1.1896301208480509e-06, accuracy=1.0\n","Test loss = 2.7751097679138184, Test accuracy = 0.752173900604248\n","Step=14399 loss=1.1459885229214706e-06, accuracy=1.0\n","Test loss = 2.8030192852020264, Test accuracy = 0.7507246136665344\n","Step=14499 loss=1.0678233462613206e-06, accuracy=1.0\n","Test loss = 2.8113057613372803, Test accuracy = 0.7507246136665344\n","Step=14599 loss=9.775320927474241e-07, accuracy=1.0\n","Test loss = 2.813037157058716, Test accuracy = 0.752173900604248\n","Step=14699 loss=9.478230026616074e-07, accuracy=1.0\n","Test loss = 2.8340532779693604, Test accuracy = 0.7507246136665344\n","Step=14799 loss=9.015736594619739e-07, accuracy=1.0\n","Test loss = 2.848090410232544, Test accuracy = 0.7507246136665344\n","Step=14899 loss=8.426306828823727e-07, accuracy=1.0\n","Test loss = 2.872079372406006, Test accuracy = 0.7507246136665344\n","Step=14999 loss=7.862393653113031e-07, accuracy=1.0\n","Test loss = 2.870734691619873, Test accuracy = 0.7507246136665344\n","Step=15099 loss=7.882324482011427e-07, accuracy=1.0\n","Test loss = 2.8999369144439697, Test accuracy = 0.7492753863334656\n","Step=15199 loss=7.178340132441008e-07, accuracy=1.0\n","Test loss = 2.8810346126556396, Test accuracy = 0.7492753863334656\n","Step=15299 loss=6.683809510832361e-07, accuracy=1.0\n","Test loss = 2.910707473754883, Test accuracy = 0.7507246136665344\n","Step=15399 loss=6.135448890631779e-07, accuracy=1.0\n","Test loss = 2.9189517498016357, Test accuracy = 0.7507246136665344\n","Step=15499 loss=6.086368335900261e-07, accuracy=1.0\n","Test loss = 2.9429454803466797, Test accuracy = 0.7507246136665344\n","Step=15599 loss=5.908859463943372e-07, accuracy=1.0\n","Test loss = 2.959197521209717, Test accuracy = 0.752173900604248\n","Step=15699 loss=5.471977601700928e-07, accuracy=1.0\n","Test loss = 2.9572489261627197, Test accuracy = 0.7507246136665344\n","Step=15799 loss=4.953884470637604e-07, accuracy=1.0\n","Test loss = 2.9424407482147217, Test accuracy = 0.7492753863334656\n","Step=15899 loss=4.7430327924757877e-07, accuracy=1.0\n","Test loss = 2.9732484817504883, Test accuracy = 0.7507246136665344\n","Step=15999 loss=4.5898313089764996e-07, accuracy=1.0\n","Test loss = 2.9791290760040283, Test accuracy = 0.7507246136665344\n","Step=16099 loss=4.198210965000726e-07, accuracy=1.0\n","Test loss = 3.008547782897949, Test accuracy = 0.752173900604248\n","Step=16199 loss=3.986149162926722e-07, accuracy=1.0\n","Test loss = 3.0142579078674316, Test accuracy = 0.7507246136665344\n","Step=16299 loss=3.941166568210974e-07, accuracy=1.0\n","Test loss = 3.0141303539276123, Test accuracy = 0.7507246136665344\n","Step=16399 loss=3.554761535440321e-07, accuracy=1.0\n","Test loss = 3.040276527404785, Test accuracy = 0.752173900604248\n","Step=16499 loss=3.477275679131253e-07, accuracy=1.0\n","Test loss = 3.0366365909576416, Test accuracy = 0.747826099395752\n","Step=16599 loss=3.1816744524348904e-07, accuracy=1.0\n","Test loss = 3.0429935455322266, Test accuracy = 0.747826099395752\n","Step=16699 loss=3.006679237671506e-07, accuracy=1.0\n","Test loss = 3.051957130432129, Test accuracy = 0.747826099395752\n","Step=16799 loss=2.851614148369208e-07, accuracy=1.0\n","Test loss = 3.0785140991210938, Test accuracy = 0.7492753863334656\n","Step=16899 loss=2.7263515619324606e-07, accuracy=1.0\n","Test loss = 3.0883874893188477, Test accuracy = 0.7492753863334656\n","Step=16999 loss=2.5148484965598074e-07, accuracy=1.0\n","Test loss = 3.0919582843780518, Test accuracy = 0.747826099395752\n","Step=17099 loss=2.3736601136192804e-07, accuracy=1.0\n","Test loss = 3.112306833267212, Test accuracy = 0.747826099395752\n","Step=17199 loss=2.3011103522208032e-07, accuracy=1.0\n","Test loss = 3.130568504333496, Test accuracy = 0.7492753863334656\n","Step=17299 loss=2.1903761606267836e-07, accuracy=1.0\n","Test loss = 3.129551649093628, Test accuracy = 0.7492753863334656\n","Step=17399 loss=2.0171503280153046e-07, accuracy=1.0\n","Test loss = 3.1458818912506104, Test accuracy = 0.7492753863334656\n","Step=17499 loss=1.870374135393149e-07, accuracy=1.0\n","Test loss = 3.1639065742492676, Test accuracy = 0.7492753863334656\n","Step=17599 loss=1.848022344574929e-07, accuracy=1.0\n","Test loss = 3.175539970397949, Test accuracy = 0.7492753863334656\n","Step=17699 loss=1.745577044687252e-07, accuracy=1.0\n","Test loss = 3.1857895851135254, Test accuracy = 0.7492753863334656\n","Step=17799 loss=1.6148194681875338e-07, accuracy=1.0\n","Test loss = 3.1959550380706787, Test accuracy = 0.7492753863334656\n","Step=17899 loss=1.6035504870615114e-07, accuracy=1.0\n","Test loss = 3.217961072921753, Test accuracy = 0.752173900604248\n","Step=17999 loss=1.4900223575864403e-07, accuracy=1.0\n","Test loss = 3.210047960281372, Test accuracy = 0.747826099395752\n","Step=18099 loss=1.3802195205414592e-07, accuracy=1.0\n","Test loss = 3.2348170280456543, Test accuracy = 0.747826099395752\n","Step=18199 loss=1.2972387203546986e-07, accuracy=1.0\n","Test loss = 3.2380943298339844, Test accuracy = 0.747826099395752\n","Step=18299 loss=1.2096013477957967e-07, accuracy=1.0\n","Test loss = 3.2496299743652344, Test accuracy = 0.747826099395752\n","Step=18399 loss=1.1066902615652907e-07, accuracy=1.0\n","Test loss = 3.2646937370300293, Test accuracy = 0.7507246136665344\n","Step=18499 loss=1.1161897539579968e-07, accuracy=1.0\n","Test loss = 3.271953821182251, Test accuracy = 0.7492753863334656\n","Step=18599 loss=1.0497865069680757e-07, accuracy=1.0\n","Test loss = 3.2879621982574463, Test accuracy = 0.7507246136665344\n","Step=18699 loss=9.498556281073433e-08, accuracy=1.0\n","Test loss = 3.2985832691192627, Test accuracy = 0.747826099395752\n","Step=18799 loss=9.55350430587032e-08, accuracy=1.0\n","Test loss = 3.3250389099121094, Test accuracy = 0.7507246136665344\n","Step=18899 loss=8.543019784923444e-08, accuracy=1.0\n","Test loss = 3.3138926029205322, Test accuracy = 0.7463768124580383\n","Step=18999 loss=8.464788738393736e-08, accuracy=1.0\n","Test loss = 3.3377532958984375, Test accuracy = 0.747826099395752\n","Step=19099 loss=8.04383105190709e-08, accuracy=1.0\n","Test loss = 3.3451218605041504, Test accuracy = 0.7492753863334656\n","Step=19199 loss=7.537191887507788e-08, accuracy=1.0\n","Test loss = 3.3544161319732666, Test accuracy = 0.7463768124580383\n","Step=19299 loss=7.01099488154e-08, accuracy=1.0\n","Test loss = 3.3707072734832764, Test accuracy = 0.7463768124580383\n","Step=19399 loss=6.433574974096246e-08, accuracy=1.0\n","Test loss = 3.3680567741394043, Test accuracy = 0.7463768124580383\n","Step=19499 loss=6.102955591558156e-08, accuracy=1.0\n","Test loss = 3.368361473083496, Test accuracy = 0.7463768124580383\n","Step=19599 loss=5.7788554670423764e-08, accuracy=1.0\n","Test loss = 3.401625156402588, Test accuracy = 0.7449275255203247\n","Step=19699 loss=5.45568664378493e-08, accuracy=1.0\n","Test loss = 3.4099528789520264, Test accuracy = 0.7463768124580383\n","Step=19799 loss=5.293636572645255e-08, accuracy=1.0\n","Test loss = 3.4374778270721436, Test accuracy = 0.7507246136665344\n","Step=19899 loss=4.7199419839216715e-08, accuracy=1.0\n","Test loss = 3.4363796710968018, Test accuracy = 0.7463768124580383\n","Step=19999 loss=4.6547494356730115e-08, accuracy=1.0\n","Test loss = 3.4556021690368652, Test accuracy = 0.7463768124580383\n","Step=20099 loss=4.2403109876687496e-08, accuracy=1.0\n","Test loss = 3.470033645629883, Test accuracy = 0.7463768124580383\n","Step=20199 loss=3.743916139775649e-08, accuracy=1.0\n","Test loss = 3.455458879470825, Test accuracy = 0.7463768124580383\n","Step=20299 loss=3.627500872127598e-08, accuracy=1.0\n","Test loss = 3.48465633392334, Test accuracy = 0.7463768124580383\n","Step=20399 loss=3.432854485119208e-08, accuracy=1.0\n","Test loss = 3.496631145477295, Test accuracy = 0.7463768124580383\n","Step=20499 loss=3.211199748776039e-08, accuracy=1.0\n","Test loss = 3.528660774230957, Test accuracy = 0.7463768124580383\n","Step=20599 loss=3.0957157779454294e-08, accuracy=1.0\n","Test loss = 3.5291965007781982, Test accuracy = 0.7463768124580383\n","Step=20699 loss=2.9234211726958394e-08, accuracy=1.0\n","Test loss = 3.5439438819885254, Test accuracy = 0.7463768124580383\n","Step=20799 loss=2.641230436140063e-08, accuracy=1.0\n","Test loss = 3.566345453262329, Test accuracy = 0.7463768124580383\n","Step=20899 loss=2.7194615439540824e-08, accuracy=1.0\n","Test loss = 3.5668375492095947, Test accuracy = 0.7463768124580383\n","Step=20999 loss=2.365559040207188e-08, accuracy=1.0\n","Test loss = 3.5743963718414307, Test accuracy = 0.7463768124580383\n","Step=21099 loss=2.2239980426164153e-08, accuracy=1.0\n","Test loss = 3.597888708114624, Test accuracy = 0.7463768124580383\n","Step=21199 loss=2.2500750613829723e-08, accuracy=1.0\n","Test loss = 3.5992767810821533, Test accuracy = 0.7463768124580383\n","Step=21299 loss=2.182088534308235e-08, accuracy=1.0\n","Test loss = 3.623230218887329, Test accuracy = 0.7449275255203247\n","Step=21399 loss=2.0051372788820742e-08, accuracy=1.0\n","Test loss = 3.637568950653076, Test accuracy = 0.7449275255203247\n","Step=21499 loss=1.7555428568627463e-08, accuracy=1.0\n","Test loss = 3.6489434242248535, Test accuracy = 0.7434782385826111\n","Step=21599 loss=1.754611525628391e-08, accuracy=1.0\n","Test loss = 3.67996883392334, Test accuracy = 0.7434782385826111\n","Step=21699 loss=1.6791744150701505e-08, accuracy=1.0\n","Test loss = 3.6778135299682617, Test accuracy = 0.7449275255203247\n","Step=21799 loss=1.587904820521402e-08, accuracy=1.0\n","Test loss = 3.6984124183654785, Test accuracy = 0.7463768124580383\n","Step=21899 loss=1.5199182752390073e-08, accuracy=1.0\n","Test loss = 3.702185869216919, Test accuracy = 0.747826099395752\n","Step=21999 loss=1.5432013449334646e-08, accuracy=1.0\n","Test loss = 3.717470645904541, Test accuracy = 0.7492753863334656\n","Step=22099 loss=1.419335444285963e-08, accuracy=1.0\n","Test loss = 3.7241158485412598, Test accuracy = 0.7492753863334656\n","Step=22199 loss=1.4305113231749544e-08, accuracy=1.0\n","Test loss = 3.754772424697876, Test accuracy = 0.7492753863334656\n","Step=22299 loss=1.2852250221850881e-08, accuracy=1.0\n","Test loss = 3.7613041400909424, Test accuracy = 0.7507246136665344\n","Step=22399 loss=1.2665985633653065e-08, accuracy=1.0\n","Test loss = 3.789097547531128, Test accuracy = 0.7507246136665344\n","Step=22499 loss=1.192092771995501e-08, accuracy=1.0\n","Test loss = 3.7988154888153076, Test accuracy = 0.7507246136665344\n","Step=22599 loss=1.2721865051412706e-08, accuracy=1.0\n","Test loss = 3.814188003540039, Test accuracy = 0.7507246136665344\n","Step=22699 loss=1.1874361616204254e-08, accuracy=1.0\n","Test loss = 3.8466365337371826, Test accuracy = 0.7507246136665344\n","Step=22799 loss=1.1464579694475675e-08, accuracy=1.0\n","Test loss = 3.8460116386413574, Test accuracy = 0.752173900604248\n","Step=22899 loss=1.16880970502109e-08, accuracy=1.0\n","Test loss = 3.8700790405273438, Test accuracy = 0.7507246136665344\n","Step=22999 loss=1.146457967449166e-08, accuracy=1.0\n","Test loss = 3.899378776550293, Test accuracy = 0.752173900604248\n","Step=23099 loss=1.1194496172262092e-08, accuracy=1.0\n","Test loss = 3.908033609390259, Test accuracy = 0.752173900604248\n","Step=23199 loss=1.108273747663091e-08, accuracy=1.0\n","Test loss = 3.958468437194824, Test accuracy = 0.7492753863334656\n","Step=23299 loss=1.1343507870797964e-08, accuracy=1.0\n","Test loss = 3.9986274242401123, Test accuracy = 0.747826099395752\n","Step=23399 loss=1.1390073896277997e-08, accuracy=1.0\n","Test loss = 4.020307540893555, Test accuracy = 0.7492753863334656\n","Step=23499 loss=1.1473892915780936e-08, accuracy=1.0\n","Test loss = 4.075927257537842, Test accuracy = 0.747826099395752\n","Step=23599 loss=1.0142101839472061e-08, accuracy=1.0\n","Test loss = 4.076589584350586, Test accuracy = 0.7492753863334656\n","Step=23699 loss=1.1399387130905935e-08, accuracy=1.0\n","Test loss = 4.132612228393555, Test accuracy = 0.7507246136665344\n","Step=23799 loss=1.0654329243853233e-08, accuracy=1.0\n","Test loss = 4.190569877624512, Test accuracy = 0.7492753863334656\n","Step=23899 loss=1.0607763145653592e-08, accuracy=1.0\n","Test loss = 4.2293829917907715, Test accuracy = 0.7492753863334656\n","Step=23999 loss=1.0095535786236453e-08, accuracy=1.0\n","Test loss = 4.2708539962768555, Test accuracy = 0.747826099395752\n","Step=24099 loss=1.0961665637410079e-08, accuracy=1.0\n","Test loss = 4.316769123077393, Test accuracy = 0.7492753863334656\n","Step=24199 loss=9.853391900738196e-09, accuracy=1.0\n","Test loss = 4.41467809677124, Test accuracy = 0.7492753863334656\n","Step=24299 loss=9.806825789215878e-09, accuracy=1.0\n","Test loss = 4.455193042755127, Test accuracy = 0.7449275255203247\n","Step=24399 loss=1.001171665127032e-08, accuracy=1.0\n","Test loss = 4.518104076385498, Test accuracy = 0.747826099395752\n","Step=24499 loss=1.0486691146560646e-08, accuracy=1.0\n","Test loss = 4.564761638641357, Test accuracy = 0.7463768124580383\n","Step=24599 loss=1.089647305163588e-08, accuracy=1.0\n","Test loss = 4.673890590667725, Test accuracy = 0.7507246136665344\n","Step=24699 loss=1.0570510142748901e-08, accuracy=1.0\n","Test loss = 4.714909076690674, Test accuracy = 0.7536231875419617\n","Step=24799 loss=1.137144746365948e-08, accuracy=1.0\n","Test loss = 4.879934310913086, Test accuracy = 0.7507246136665344\n","Step=24899 loss=1.0272486826168326e-08, accuracy=1.0\n","Test loss = 4.964978218078613, Test accuracy = 0.7492753863334656\n","Step=24999 loss=1.017004147785272e-08, accuracy=1.0\n","Test loss = 5.031339645385742, Test accuracy = 0.747826099395752\n","Step=25099 loss=1.0700895029525093e-08, accuracy=1.0\n","Test loss = 5.222945213317871, Test accuracy = 0.7449275255203247\n","Step=25199 loss=1.0216607585489257e-08, accuracy=1.0\n","Test loss = 5.302089214324951, Test accuracy = 0.7449275255203247\n","Step=25299 loss=1.0738148188416119e-08, accuracy=1.0\n","Test loss = 5.332958698272705, Test accuracy = 0.7449275255203247\n","Step=25399 loss=9.452923273034485e-09, accuracy=1.0\n","Test loss = 5.514886379241943, Test accuracy = 0.7492753863334656\n","Step=25499 loss=9.713693516211208e-09, accuracy=1.0\n","Test loss = 5.617150783538818, Test accuracy = 0.7536231875419617\n","Step=25599 loss=1.1045484570315622e-08, accuracy=1.0\n","Test loss = 5.874348163604736, Test accuracy = 0.7550724744796753\n","Step=25699 loss=1.0244547291038408e-08, accuracy=1.0\n","Test loss = 6.0811991691589355, Test accuracy = 0.7579709887504578\n","Step=25799 loss=1.088715973929233e-08, accuracy=1.0\n","Test loss = 6.332975387573242, Test accuracy = 0.7565217614173889\n","Step=25899 loss=1.0905786133053042e-08, accuracy=1.0\n","Test loss = 6.602254390716553, Test accuracy = 0.7565217614173889\n","Step=25999 loss=1.2656671729560643e-08, accuracy=1.0\n","Test loss = 6.704670429229736, Test accuracy = 0.7550724744796753\n","Step=26099 loss=9.294597715925335e-09, accuracy=1.0\n","Test loss = 7.13360071182251, Test accuracy = 0.7492753863334656\n","Step=26199 loss=1.257285153488663e-08, accuracy=1.0\n","Test loss = 7.352400302886963, Test accuracy = 0.7565217614173889\n","Step=26299 loss=0.004241686093382156, accuracy=0.9990625\n","Test loss = 7.178105354309082, Test accuracy = 0.7637681365013123\n","Step=26399 loss=6.899211812196882e-06, accuracy=1.0\n","Test loss = 7.529536724090576, Test accuracy = 0.7550724744796753\n","Step=26499 loss=2.7038245778499005e-06, accuracy=1.0\n","Test loss = 7.524776935577393, Test accuracy = 0.7550724744796753\n","Step=26599 loss=1.833185749973154e-06, accuracy=1.0\n","Test loss = 7.527403354644775, Test accuracy = 0.7565217614173889\n","Step=26699 loss=1.6408159011405132e-06, accuracy=1.0\n","Test loss = 7.532039642333984, Test accuracy = 0.7565217614173889\n","Step=26799 loss=1.3657825456370177e-06, accuracy=1.0\n","Test loss = 7.533401966094971, Test accuracy = 0.7565217614173889\n","Step=26899 loss=1.1973143169541346e-06, accuracy=1.0\n","Test loss = 7.538893222808838, Test accuracy = 0.7565217614173889\n","Step=26999 loss=1.0032242268209757e-06, accuracy=1.0\n","Test loss = 7.540047645568848, Test accuracy = 0.7565217614173889\n","Step=27099 loss=9.422753642240877e-07, accuracy=1.0\n","Test loss = 7.546051502227783, Test accuracy = 0.7565217614173889\n","Step=27199 loss=9.210454825847591e-07, accuracy=1.0\n","Test loss = 7.550004482269287, Test accuracy = 0.7565217614173889\n","Step=27299 loss=7.957496040944534e-07, accuracy=1.0\n","Test loss = 7.554537296295166, Test accuracy = 0.7565217614173889\n","Step=27399 loss=6.636018645878039e-07, accuracy=1.0\n","Test loss = 7.55714750289917, Test accuracy = 0.7565217614173889\n","Step=27499 loss=6.513750530245943e-07, accuracy=1.0\n","Test loss = 7.558624267578125, Test accuracy = 0.7565217614173889\n","Step=27599 loss=5.976586312783638e-07, accuracy=1.0\n","Test loss = 7.555893421173096, Test accuracy = 0.7565217614173889\n","Step=27699 loss=6.318650891046218e-07, accuracy=1.0\n","Test loss = 7.561181545257568, Test accuracy = 0.7565217614173889\n","Step=27799 loss=5.005058910967363e-07, accuracy=1.0\n","Test loss = 7.558469295501709, Test accuracy = 0.7565217614173889\n","Step=27899 loss=4.990342937816194e-07, accuracy=1.0\n","Test loss = 7.557664394378662, Test accuracy = 0.7565217614173889\n","Step=27999 loss=4.861547033385705e-07, accuracy=1.0\n","Test loss = 7.557387828826904, Test accuracy = 0.7565217614173889\n","Step=28099 loss=4.7341431610448124e-07, accuracy=1.0\n","Test loss = 7.5558905601501465, Test accuracy = 0.7565217614173889\n","Step=28199 loss=4.4558720073695214e-07, accuracy=1.0\n","Test loss = 7.557775497436523, Test accuracy = 0.7565217614173889\n","Step=28299 loss=4.393193409413243e-07, accuracy=1.0\n","Test loss = 7.563011646270752, Test accuracy = 0.7565217614173889\n","Step=28399 loss=4.2766883737499486e-07, accuracy=1.0\n","Test loss = 7.568088531494141, Test accuracy = 0.7565217614173889\n","Step=28499 loss=3.828639022174229e-07, accuracy=1.0\n","Test loss = 7.569911956787109, Test accuracy = 0.7565217614173889\n","Step=28599 loss=3.837953381236048e-07, accuracy=1.0\n","Test loss = 7.57075834274292, Test accuracy = 0.7565217614173889\n","Step=28699 loss=3.4848937517040215e-07, accuracy=1.0\n","Test loss = 7.569161891937256, Test accuracy = 0.7565217614173889\n","Step=28799 loss=3.091786821585174e-07, accuracy=1.0\n","Test loss = 7.566919326782227, Test accuracy = 0.7565217614173889\n","Step=28899 loss=3.1883633468510196e-07, accuracy=1.0\n","Test loss = 7.569516181945801, Test accuracy = 0.7565217614173889\n","Step=28999 loss=3.0251060778141437e-07, accuracy=1.0\n","Test loss = 7.567113876342773, Test accuracy = 0.7565217614173889\n","Step=29099 loss=2.790693182674886e-07, accuracy=1.0\n","Test loss = 7.566878318786621, Test accuracy = 0.7565217614173889\n","Step=29199 loss=2.694303317696267e-07, accuracy=1.0\n","Test loss = 7.565825939178467, Test accuracy = 0.7565217614173889\n","Step=29299 loss=2.553488145551341e-07, accuracy=1.0\n","Test loss = 7.5623555183410645, Test accuracy = 0.7565217614173889\n","Step=29399 loss=2.5354213992301314e-07, accuracy=1.0\n","Test loss = 7.559727191925049, Test accuracy = 0.7579709887504578\n","Step=29499 loss=2.5619632708639984e-07, accuracy=1.0\n","Test loss = 7.565074920654297, Test accuracy = 0.7565217614173889\n","Step=29599 loss=2.3026858592345434e-07, accuracy=1.0\n","Test loss = 7.563356876373291, Test accuracy = 0.7579709887504578\n","Step=29699 loss=2.416771808100293e-07, accuracy=1.0\n","Test loss = 7.565329074859619, Test accuracy = 0.7579709887504578\n","Step=29799 loss=2.2777268839035968e-07, accuracy=1.0\n","Test loss = 7.568153381347656, Test accuracy = 0.7579709887504578\n","Step=29899 loss=2.179380985012358e-07, accuracy=1.0\n","Test loss = 7.565207004547119, Test accuracy = 0.7579709887504578\n","Step=29999 loss=1.828367028622324e-07, accuracy=1.0\n","Test loss = 7.560822010040283, Test accuracy = 0.7594202756881714\n","Step=30099 loss=2.023663694927791e-07, accuracy=1.0\n","Test loss = 7.564491271972656, Test accuracy = 0.7579709887504578\n","Step=30199 loss=1.8429891667892662e-07, accuracy=1.0\n","Test loss = 7.560740947723389, Test accuracy = 0.7594202756881714\n","Step=30299 loss=1.902220689942169e-07, accuracy=1.0\n","Test loss = 7.558355808258057, Test accuracy = 0.7594202756881714\n","Step=30399 loss=1.7181921819542366e-07, accuracy=1.0\n","Test loss = 7.56040096282959, Test accuracy = 0.7594202756881714\n","Step=30499 loss=1.7628025410942883e-07, accuracy=1.0\n","Test loss = 7.5604777336120605, Test accuracy = 0.7594202756881714\n","Step=30599 loss=1.4687854143602408e-07, accuracy=1.0\n","Test loss = 7.556966781616211, Test accuracy = 0.7594202756881714\n","Step=30699 loss=1.4803336089386222e-07, accuracy=1.0\n","Test loss = 7.557392120361328, Test accuracy = 0.7594202756881714\n","Step=30799 loss=1.6100664097962182e-07, accuracy=1.0\n","Test loss = 7.561562538146973, Test accuracy = 0.7579709887504578\n","Step=30899 loss=1.4418707067420655e-07, accuracy=1.0\n","Test loss = 7.558483123779297, Test accuracy = 0.7579709887504578\n","Step=30999 loss=1.339425551094564e-07, accuracy=1.0\n","Test loss = 7.551465034484863, Test accuracy = 0.7594202756881714\n","Step=31099 loss=1.4693445059776878e-07, accuracy=1.0\n","Test loss = 7.553012371063232, Test accuracy = 0.7594202756881714\n","Step=31199 loss=1.3231274024860794e-07, accuracy=1.0\n","Test loss = 7.549257755279541, Test accuracy = 0.7594202756881714\n","Step=31299 loss=1.364384900526261e-07, accuracy=1.0\n","Test loss = 7.553259372711182, Test accuracy = 0.7594202756881714\n","Step=31399 loss=1.4317194256818767e-07, accuracy=1.0\n","Test loss = 7.5562543869018555, Test accuracy = 0.7579709887504578\n","Step=31499 loss=1.2095066800554833e-07, accuracy=1.0\n","Test loss = 7.557825088500977, Test accuracy = 0.7579709887504578\n","Step=31599 loss=1.1312758703141413e-07, accuracy=1.0\n","Test loss = 7.554428577423096, Test accuracy = 0.7579709887504578\n","Step=31699 loss=1.1803563179313415e-07, accuracy=1.0\n","Test loss = 7.55483341217041, Test accuracy = 0.7579709887504578\n","Step=31799 loss=1.1019393014066737e-07, accuracy=1.0\n","Test loss = 7.55409574508667, Test accuracy = 0.7579709887504578\n","Step=31899 loss=1.1340699114015252e-07, accuracy=1.0\n","Test loss = 7.55276346206665, Test accuracy = 0.7579709887504578\n","Step=31999 loss=1.1114389216970722e-07, accuracy=1.0\n","Test loss = 7.554269313812256, Test accuracy = 0.7579709887504578\n","Step=32099 loss=9.355125026644372e-08, accuracy=1.0\n","Test loss = 7.547780990600586, Test accuracy = 0.7579709887504578\n","Step=32199 loss=1.0364676708007892e-07, accuracy=1.0\n","Test loss = 7.54332971572876, Test accuracy = 0.7594202756881714\n","Step=32299 loss=9.456638929705719e-08, accuracy=1.0\n","Test loss = 7.541562557220459, Test accuracy = 0.7594202756881714\n","Step=32399 loss=9.208907897573226e-08, accuracy=1.0\n","Test loss = 7.539979457855225, Test accuracy = 0.7594202756881714\n","Step=32499 loss=8.784225311586624e-08, accuracy=1.0\n","Test loss = 7.539276123046875, Test accuracy = 0.7594202756881714\n","Step=32599 loss=8.969558372484698e-08, accuracy=1.0\n","Test loss = 7.540075302124023, Test accuracy = 0.7579709887504578\n","Step=32699 loss=8.509486157493029e-08, accuracy=1.0\n","Test loss = 7.539670944213867, Test accuracy = 0.7579709887504578\n","Step=32799 loss=9.016124826288774e-08, accuracy=1.0\n","Test loss = 7.541505336761475, Test accuracy = 0.7579709887504578\n","Step=32899 loss=8.498310636539941e-08, accuracy=1.0\n","Test loss = 7.541918754577637, Test accuracy = 0.7579709887504578\n","Step=32999 loss=7.97025154852804e-08, accuracy=1.0\n","Test loss = 7.540460586547852, Test accuracy = 0.7579709887504578\n","Step=33099 loss=7.583753385276281e-08, accuracy=1.0\n","Test loss = 7.540098667144775, Test accuracy = 0.7579709887504578\n","Step=33199 loss=7.58841028414281e-08, accuracy=1.0\n","Test loss = 7.539665222167969, Test accuracy = 0.7579709887504578\n","Step=33299 loss=7.809133727221251e-08, accuracy=1.0\n","Test loss = 7.542413711547852, Test accuracy = 0.7579709887504578\n","Step=33399 loss=7.40028346335464e-08, accuracy=1.0\n","Test loss = 7.545660495758057, Test accuracy = 0.7579709887504578\n","Step=33499 loss=7.076184083132375e-08, accuracy=1.0\n","Test loss = 7.541962623596191, Test accuracy = 0.7579709887504578\n","Step=33599 loss=6.908546366091172e-08, accuracy=1.0\n","Test loss = 7.541194438934326, Test accuracy = 0.7579709887504578\n","Step=33699 loss=6.553713049584076e-08, accuracy=1.0\n","Test loss = 7.536872863769531, Test accuracy = 0.7579709887504578\n","Step=33799 loss=6.204467199566466e-08, accuracy=1.0\n","Test loss = 7.533719062805176, Test accuracy = 0.7579709887504578\n","Step=33899 loss=6.062906214410191e-08, accuracy=1.0\n","Test loss = 7.53367280960083, Test accuracy = 0.7579709887504578\n","Step=33999 loss=6.085258167587427e-08, accuracy=1.0\n","Test loss = 7.532257556915283, Test accuracy = 0.7579709887504578\n","Step=34099 loss=6.19887957498122e-08, accuracy=1.0\n","Test loss = 7.531959056854248, Test accuracy = 0.7579709887504578\n","Step=34199 loss=5.791891743101019e-08, accuracy=1.0\n","Test loss = 7.531906604766846, Test accuracy = 0.7579709887504578\n","Step=34299 loss=5.494800404903799e-08, accuracy=1.0\n","Test loss = 7.528875350952148, Test accuracy = 0.7579709887504578\n","Step=34399 loss=5.5255339397319855e-08, accuracy=1.0\n","Test loss = 7.529301166534424, Test accuracy = 0.7579709887504578\n","Step=34499 loss=5.4798990833937464e-08, accuracy=1.0\n","Test loss = 7.531192779541016, Test accuracy = 0.7579709887504578\n","Step=34599 loss=4.803759795279916e-08, accuracy=1.0\n","Test loss = 7.525712013244629, Test accuracy = 0.7579709887504578\n","Step=34699 loss=4.8065536653041365e-08, accuracy=1.0\n","Test loss = 7.523233890533447, Test accuracy = 0.7579709887504578\n","Step=34799 loss=4.8484630408296425e-08, accuracy=1.0\n","Test loss = 7.524458408355713, Test accuracy = 0.7579709887504578\n","Step=34899 loss=4.988161437680105e-08, accuracy=1.0\n","Test loss = 7.529098033905029, Test accuracy = 0.7579709887504578\n","Step=34999 loss=4.340892926002482e-08, accuracy=1.0\n","Test loss = 7.522453784942627, Test accuracy = 0.7579709887504578\n","Step=35099 loss=4.671512066067862e-08, accuracy=1.0\n","Test loss = 7.52760648727417, Test accuracy = 0.7579709887504578\n","Step=35199 loss=4.51970664894219e-08, accuracy=1.0\n","Test loss = 7.531821250915527, Test accuracy = 0.7579709887504578\n","Step=35299 loss=4.054977035039542e-08, accuracy=1.0\n","Test loss = 7.526783466339111, Test accuracy = 0.7579709887504578\n","Step=35399 loss=4.0633589577510064e-08, accuracy=1.0\n","Test loss = 7.527956485748291, Test accuracy = 0.7579709887504578\n","Step=35499 loss=3.7988633803109904e-08, accuracy=1.0\n","Test loss = 7.529440402984619, Test accuracy = 0.7579709887504578\n","Step=35599 loss=3.721563715863851e-08, accuracy=1.0\n","Test loss = 7.530814170837402, Test accuracy = 0.7579709887504578\n","Step=35699 loss=3.679654295929424e-08, accuracy=1.0\n","Test loss = 7.531982898712158, Test accuracy = 0.7579709887504578\n","Step=35799 loss=3.3862877799606395e-08, accuracy=1.0\n","Test loss = 7.525496959686279, Test accuracy = 0.7579709887504578\n","Step=35899 loss=3.55858242429008e-08, accuracy=1.0\n","Test loss = 7.529865741729736, Test accuracy = 0.7579709887504578\n","Step=35999 loss=3.508291019471699e-08, accuracy=1.0\n","Test loss = 7.532862186431885, Test accuracy = 0.7579709887504578\n","Step=36099 loss=3.302468896571042e-08, accuracy=1.0\n","Test loss = 7.526464939117432, Test accuracy = 0.7579709887504578\n","Step=36199 loss=3.0798829540046315e-08, accuracy=1.0\n","Test loss = 7.531081676483154, Test accuracy = 0.7579709887504578\n","Step=36299 loss=3.122723787329917e-08, accuracy=1.0\n","Test loss = 7.529565811157227, Test accuracy = 0.7579709887504578\n","Step=36399 loss=2.9029317349937855e-08, accuracy=1.0\n","Test loss = 7.528542518615723, Test accuracy = 0.7579709887504578\n","Step=36499 loss=2.892687188005283e-08, accuracy=1.0\n","Test loss = 7.5360236167907715, Test accuracy = 0.7579709887504578\n","Step=36599 loss=2.875923422962501e-08, accuracy=1.0\n","Test loss = 7.53505802154541, Test accuracy = 0.7579709887504578\n","Step=36699 loss=2.725980563056396e-08, accuracy=1.0\n","Test loss = 7.541326522827148, Test accuracy = 0.7579709887504578\n","Step=36799 loss=2.5359908102373653e-08, accuracy=1.0\n","Test loss = 7.530151844024658, Test accuracy = 0.7579709887504578\n","Step=36899 loss=2.6756890951773472e-08, accuracy=1.0\n","Test loss = 7.535062313079834, Test accuracy = 0.7579709887504578\n","Step=36999 loss=2.27056395729619e-08, accuracy=1.0\n","Test loss = 7.523042678833008, Test accuracy = 0.7565217614173889\n","Step=37099 loss=2.0880247610932656e-08, accuracy=1.0\n","Test loss = 7.5240559577941895, Test accuracy = 0.7565217614173889\n","Step=37199 loss=2.2724266135476512e-08, accuracy=1.0\n","Test loss = 7.537158012390137, Test accuracy = 0.7579709887504578\n","Step=37299 loss=2.071260984504164e-08, accuracy=1.0\n","Test loss = 7.534489154815674, Test accuracy = 0.7579709887504578\n","Step=37399 loss=1.9790600536140347e-08, accuracy=1.0\n","Test loss = 7.540286540985107, Test accuracy = 0.7579709887504578\n","Step=37499 loss=1.803971484193312e-08, accuracy=1.0\n","Test loss = 7.54121732711792, Test accuracy = 0.7579709887504578\n","Step=37599 loss=1.940875868466918e-08, accuracy=1.0\n","Test loss = 7.5474162101745605, Test accuracy = 0.7579709887504578\n","Step=37699 loss=1.7527487785606864e-08, accuracy=1.0\n","Test loss = 7.547857761383057, Test accuracy = 0.7579709887504578\n","Step=37799 loss=1.5096735928032955e-08, accuracy=1.0\n","Test loss = 7.543201446533203, Test accuracy = 0.7565217614173889\n","Step=37899 loss=1.588836019639217e-08, accuracy=1.0\n","Test loss = 7.545139789581299, Test accuracy = 0.7565217614173889\n","Step=37999 loss=1.5106049691682167e-08, accuracy=1.0\n","Test loss = 7.546600341796875, Test accuracy = 0.7565217614173889\n","Step=38099 loss=1.5180555579252798e-08, accuracy=1.0\n","Test loss = 7.546462535858154, Test accuracy = 0.7565217614173889\n","Step=38199 loss=1.4845279379116505e-08, accuracy=1.0\n","Test loss = 7.551304817199707, Test accuracy = 0.7565217614173889\n","Step=38299 loss=1.4631075252458103e-08, accuracy=1.0\n","Test loss = 7.558531284332275, Test accuracy = 0.7565217614173889\n","Step=38399 loss=1.481733983732525e-08, accuracy=1.0\n","Test loss = 7.5572123527526855, Test accuracy = 0.7565217614173889\n","Step=38499 loss=1.2824309882364383e-08, accuracy=1.0\n","Test loss = 7.554937839508057, Test accuracy = 0.7565217614173889\n","Step=38599 loss=1.3569367849353143e-08, accuracy=1.0\n","Test loss = 7.549338340759277, Test accuracy = 0.7565217614173889\n","Step=38699 loss=1.2945381835383075e-08, accuracy=1.0\n","Test loss = 7.555295467376709, Test accuracy = 0.7565217614173889\n","Step=38799 loss=1.236796196124601e-08, accuracy=1.0\n","Test loss = 7.559767723083496, Test accuracy = 0.7579709887504578\n","Step=38899 loss=1.2777743987335555e-08, accuracy=1.0\n","Test loss = 7.5639214515686035, Test accuracy = 0.7579709887504578\n","Step=38999 loss=1.1045484178406895e-08, accuracy=1.0\n","Test loss = 7.570208549499512, Test accuracy = 0.7579709887504578\n","Step=39099 loss=1.1352820776799888e-08, accuracy=1.0\n","Test loss = 7.570876598358154, Test accuracy = 0.7579709887504578\n","Step=39199 loss=1.0961665340980531e-08, accuracy=1.0\n","Test loss = 7.574941158294678, Test accuracy = 0.7579709887504578\n","Step=39299 loss=1.0123475092660428e-08, accuracy=1.0\n","Test loss = 7.580877780914307, Test accuracy = 0.7579709887504578\n","Step=39399 loss=1.0151414810977144e-08, accuracy=1.0\n","Test loss = 7.581639766693115, Test accuracy = 0.7579709887504578\n","Step=39499 loss=1.0551883342646562e-08, accuracy=1.0\n","Test loss = 7.5800065994262695, Test accuracy = 0.7579709887504578\n","Step=39599 loss=9.257345318092191e-09, accuracy=1.0\n","Test loss = 7.5876007080078125, Test accuracy = 0.7579709887504578\n","Step=39699 loss=9.49017595730517e-09, accuracy=1.0\n","Test loss = 7.595652103424072, Test accuracy = 0.7579709887504578\n","Step=39799 loss=9.592621441623095e-09, accuracy=1.0\n","Test loss = 7.609353542327881, Test accuracy = 0.7579709887504578\n","Step=39899 loss=8.754431237156801e-09, accuracy=1.0\n","Test loss = 7.60935115814209, Test accuracy = 0.7579709887504578\n","Step=39999 loss=8.177011340260166e-09, accuracy=1.0\n","Test loss = 7.616160869598389, Test accuracy = 0.7579709887504578\n","Step=40099 loss=7.487832730523891e-09, accuracy=1.0\n","Test loss = 7.630890369415283, Test accuracy = 0.7579709887504578\n","Step=40199 loss=8.111818798894887e-09, accuracy=1.0\n","Test loss = 7.634917259216309, Test accuracy = 0.7579709887504578\n","Step=40299 loss=8.037312927977602e-09, accuracy=1.0\n","Test loss = 7.643266201019287, Test accuracy = 0.7594202756881714\n","Step=40399 loss=7.050111108797274e-09, accuracy=1.0\n","Test loss = 7.653177261352539, Test accuracy = 0.7594202756881714\n","Step=40499 loss=7.180496322534147e-09, accuracy=1.0\n","Test loss = 7.664114952087402, Test accuracy = 0.7594202756881714\n","Step=40599 loss=6.267800286563485e-09, accuracy=1.0\n","Test loss = 7.670246124267578, Test accuracy = 0.7594202756881714\n","Step=40699 loss=6.956978886307752e-09, accuracy=1.0\n","Test loss = 7.676212787628174, Test accuracy = 0.7594202756881714\n","Step=40799 loss=6.752087924888351e-09, accuracy=1.0\n","Test loss = 7.686115264892578, Test accuracy = 0.7579709887504578\n","Step=40899 loss=6.817280519544333e-09, accuracy=1.0\n","Test loss = 7.692573070526123, Test accuracy = 0.7579709887504578\n","Step=40999 loss=6.631016050695493e-09, accuracy=1.0\n","Test loss = 7.698558330535889, Test accuracy = 0.7579709887504578\n","Step=41099 loss=5.476176181962167e-09, accuracy=1.0\n","Test loss = 7.706997871398926, Test accuracy = 0.7579709887504578\n","Step=41199 loss=6.249173812866715e-09, accuracy=1.0\n","Test loss = 7.715917110443115, Test accuracy = 0.7579709887504578\n","Step=41299 loss=4.833563695405551e-09, accuracy=1.0\n","Test loss = 7.727472305297852, Test accuracy = 0.7579709887504578\n","Step=41399 loss=5.858018422566502e-09, accuracy=1.0\n","Test loss = 7.741593837738037, Test accuracy = 0.7579709887504578\n","Step=41499 loss=5.420296851910145e-09, accuracy=1.0\n","Test loss = 7.7436113357543945, Test accuracy = 0.7579709887504578\n","Step=41599 loss=5.6158745953549526e-09, accuracy=1.0\n","Test loss = 7.767141342163086, Test accuracy = 0.7579709887504578\n","Step=41699 loss=6.426125138681016e-09, accuracy=1.0\n","Test loss = 7.78413724899292, Test accuracy = 0.7579709887504578\n","Step=41799 loss=5.40167043872053e-09, accuracy=1.0\n","Test loss = 7.794056415557861, Test accuracy = 0.7579709887504578\n","Step=41899 loss=5.345791104227615e-09, accuracy=1.0\n","Test loss = 7.808324813842773, Test accuracy = 0.7579709887504578\n","Step=41999 loss=5.671753954827885e-09, accuracy=1.0\n","Test loss = 7.832281112670898, Test accuracy = 0.7579709887504578\n","Step=42099 loss=5.876644881275262e-09, accuracy=1.0\n","Test loss = 7.850762844085693, Test accuracy = 0.7579709887504578\n","Step=42199 loss=6.4633780760825485e-09, accuracy=1.0\n","Test loss = 7.861485958099365, Test accuracy = 0.7579709887504578\n","Step=42299 loss=6.146728412925739e-09, accuracy=1.0\n","Test loss = 7.88673734664917, Test accuracy = 0.7550724744796753\n","Step=42399 loss=5.932524295704233e-09, accuracy=1.0\n","Test loss = 7.897895336151123, Test accuracy = 0.7550724744796753\n","Step=42499 loss=5.830078780300063e-09, accuracy=1.0\n","Test loss = 7.888057708740234, Test accuracy = 0.7550724744796753\n","Step=42599 loss=7.729976602699474e-09, accuracy=1.0\n","Test loss = 7.919197082519531, Test accuracy = 0.7550724744796753\n","Step=42699 loss=6.975605422732123e-09, accuracy=1.0\n","Test loss = 7.942977428436279, Test accuracy = 0.7550724744796753\n","Step=42799 loss=6.891786361595819e-09, accuracy=1.0\n","Test loss = 7.931214809417725, Test accuracy = 0.7536231875419617\n","Step=42899 loss=7.841735204516808e-09, accuracy=1.0\n","Test loss = 7.9729790687561035, Test accuracy = 0.7550724744796753\n","Step=42999 loss=7.627531121712217e-09, accuracy=1.0\n","Test loss = 7.979466915130615, Test accuracy = 0.7536231875419617\n","Step=43099 loss=7.618217918725634e-09, accuracy=1.0\n","Test loss = 8.026971817016602, Test accuracy = 0.752173900604248\n","Step=43199 loss=7.851048566265284e-09, accuracy=1.0\n","Test loss = 8.007887840270996, Test accuracy = 0.7492753863334656\n","Step=43299 loss=7.115303744531509e-09, accuracy=1.0\n","Test loss = 8.041252136230469, Test accuracy = 0.7492753863334656\n","Step=43399 loss=7.944180762109455e-09, accuracy=1.0\n","Test loss = 8.063258171081543, Test accuracy = 0.7507246136665344\n","Step=43499 loss=8.437781693348967e-09, accuracy=1.0\n","Test loss = 8.100915908813477, Test accuracy = 0.752173900604248\n","Step=43599 loss=7.888301440939216e-09, accuracy=1.0\n","Test loss = 8.135456085205078, Test accuracy = 0.7507246136665344\n","Step=43699 loss=8.624046197169833e-09, accuracy=1.0\n","Test loss = 8.15567684173584, Test accuracy = 0.7507246136665344\n","Step=43799 loss=8.866190011058706e-09, accuracy=1.0\n","Test loss = 8.202999114990234, Test accuracy = 0.7507246136665344\n","Step=43899 loss=9.508802538138462e-09, accuracy=1.0\n","Test loss = 8.242340087890625, Test accuracy = 0.752173900604248\n","Step=43999 loss=1.0598449800003351e-08, accuracy=1.0\n","Test loss = 8.283605575561523, Test accuracy = 0.752173900604248\n","Step=44099 loss=9.723006633710618e-09, accuracy=1.0\n","Test loss = 8.30407428741455, Test accuracy = 0.7507246136665344\n","Step=44199 loss=1.0477377815343303e-08, accuracy=1.0\n","Test loss = 8.35859489440918, Test accuracy = 0.7507246136665344\n","Step=44299 loss=1.149251939058793e-08, accuracy=1.0\n","Test loss = 8.392991065979004, Test accuracy = 0.752173900604248\n","Step=44399 loss=1.155771188143806e-08, accuracy=1.0\n","Test loss = 8.455748558044434, Test accuracy = 0.7536231875419617\n","Step=44499 loss=1.0859220123116132e-08, accuracy=1.0\n","Test loss = 8.525940895080566, Test accuracy = 0.7536231875419617\n","Step=44599 loss=1.0542570445526422e-08, accuracy=1.0\n","Test loss = 8.554786682128906, Test accuracy = 0.7550724744796753\n","Step=44699 loss=1.1399387096489021e-08, accuracy=1.0\n","Test loss = 8.614062309265137, Test accuracy = 0.7550724744796753\n","Step=44799 loss=1.0067596004637025e-08, accuracy=1.0\n","Test loss = 8.647048950195312, Test accuracy = 0.7550724744796753\n","Step=44899 loss=1.0589136512084707e-08, accuracy=1.0\n","Test loss = 8.68663215637207, Test accuracy = 0.7550724744796753\n","Step=44999 loss=1.1576338402319308e-08, accuracy=1.0\n","Test loss = 8.812339782714844, Test accuracy = 0.7565217614173889\n","Step=45099 loss=1.223757725488106e-08, accuracy=1.0\n","Test loss = 8.786913871765137, Test accuracy = 0.7550724744796753\n","Step=45199 loss=1.1064111042902169e-08, accuracy=1.0\n","Test loss = 8.958585739135742, Test accuracy = 0.7579709887504578\n","Step=45299 loss=1.1976807066105267e-08, accuracy=1.0\n","Test loss = 9.011758804321289, Test accuracy = 0.7565217614173889\n","Step=45399 loss=1.1641530892614326e-08, accuracy=1.0\n","Test loss = 9.11025619506836, Test accuracy = 0.7579709887504578\n","Step=45499 loss=1.287087648238483e-08, accuracy=1.0\n","Test loss = 9.196944236755371, Test accuracy = 0.7579709887504578\n","Step=45599 loss=1.2191011240503258e-08, accuracy=1.0\n","Test loss = 9.315075874328613, Test accuracy = 0.7550724744796753\n","Step=45699 loss=1.3979149324216955e-08, accuracy=1.0\n","Test loss = 9.471231460571289, Test accuracy = 0.7550724744796753\n","Step=45799 loss=1.2433154895075126e-08, accuracy=1.0\n","Test loss = 9.522961616516113, Test accuracy = 0.7536231875419617\n","Step=45899 loss=1.2638045875923431e-08, accuracy=1.0\n","Test loss = 9.641200065612793, Test accuracy = 0.7536231875419617\n","Step=45999 loss=1.2796370590373307e-08, accuracy=1.0\n","Test loss = 9.687043190002441, Test accuracy = 0.7550724744796753\n","Step=46099 loss=1.2777743452763169e-08, accuracy=1.0\n","Test loss = 9.913067817687988, Test accuracy = 0.7536231875419617\n","Step=46199 loss=2.015381358244639e-08, accuracy=1.0\n","Test loss = 10.289299011230469, Test accuracy = 0.7507246136665344\n","Step=46299 loss=1.0812653838399022e-08, accuracy=1.0\n","Test loss = 10.361226081848145, Test accuracy = 0.7550724744796753\n","Step=46399 loss=1.4100221379931276e-08, accuracy=1.0\n","Test loss = 10.514110565185547, Test accuracy = 0.7536231875419617\n","Step=46499 loss=2.1075820498817066e-08, accuracy=1.0\n","Test loss = 10.682717323303223, Test accuracy = 0.7507246136665344\n","Step=46599 loss=0.06483523551758434, accuracy=0.991640625\n","Test loss = 10.430898666381836, Test accuracy = 0.7550724744796753\n","Step=46699 loss=1.3690848263614442e-06, accuracy=1.0\n","Test loss = 10.762250900268555, Test accuracy = 0.7492753863334656\n","Step=46799 loss=1.194351699003171e-06, accuracy=1.0\n","Test loss = 10.744344711303711, Test accuracy = 0.7492753863334656\n","Step=46899 loss=1.0392027608174814e-06, accuracy=1.0\n","Test loss = 10.728863716125488, Test accuracy = 0.7492753863334656\n","Step=46999 loss=9.442651327340457e-07, accuracy=1.0\n","Test loss = 10.714471817016602, Test accuracy = 0.7507246136665344\n","Step=47099 loss=8.241694830513069e-07, accuracy=1.0\n","Test loss = 10.702132225036621, Test accuracy = 0.7507246136665344\n","Step=47199 loss=6.949270025202736e-07, accuracy=1.0\n","Test loss = 10.692686080932617, Test accuracy = 0.7507246136665344\n","Step=47299 loss=7.254942117329577e-07, accuracy=1.0\n","Test loss = 10.68205451965332, Test accuracy = 0.752173900604248\n","Step=47399 loss=6.025171014112374e-07, accuracy=1.0\n","Test loss = 10.672500610351562, Test accuracy = 0.752173900604248\n","Step=47499 loss=5.0633298066316e-07, accuracy=1.0\n","Test loss = 10.665472030639648, Test accuracy = 0.752173900604248\n","Step=47599 loss=4.437498457043887e-07, accuracy=1.0\n","Test loss = 10.659482955932617, Test accuracy = 0.7536231875419617\n","Step=47699 loss=4.283739610311255e-07, accuracy=1.0\n","Test loss = 10.654553413391113, Test accuracy = 0.7536231875419617\n","Step=47799 loss=4.931563079024759e-07, accuracy=1.0\n","Test loss = 10.64799976348877, Test accuracy = 0.7536231875419617\n","Step=47899 loss=3.602592307816455e-07, accuracy=1.0\n","Test loss = 10.642991065979004, Test accuracy = 0.7536231875419617\n","Step=47999 loss=3.8412891416150787e-07, accuracy=1.0\n","Test loss = 10.637076377868652, Test accuracy = 0.7536231875419617\n","Step=48099 loss=3.7938856884300875e-07, accuracy=1.0\n","Test loss = 10.632158279418945, Test accuracy = 0.7536231875419617\n","Step=48199 loss=3.762316401001442e-07, accuracy=1.0\n","Test loss = 10.62708568572998, Test accuracy = 0.7536231875419617\n","Step=48299 loss=3.118318024719624e-07, accuracy=1.0\n","Test loss = 10.62250804901123, Test accuracy = 0.7536231875419617\n","Step=48399 loss=3.323672630706209e-07, accuracy=1.0\n","Test loss = 10.618401527404785, Test accuracy = 0.7536231875419617\n","Step=48499 loss=3.4841383827144413e-07, accuracy=1.0\n","Test loss = 10.613954544067383, Test accuracy = 0.7536231875419617\n","Step=48599 loss=3.0209071635978193e-07, accuracy=1.0\n","Test loss = 10.608610153198242, Test accuracy = 0.7536231875419617\n","Step=48699 loss=2.894340050541011e-07, accuracy=1.0\n","Test loss = 10.604588508605957, Test accuracy = 0.7536231875419617\n","Step=48799 loss=2.842744825848342e-07, accuracy=1.0\n","Test loss = 10.601950645446777, Test accuracy = 0.7536231875419617\n","Step=48899 loss=2.427101292362677e-07, accuracy=1.0\n","Test loss = 10.598725318908691, Test accuracy = 0.7536231875419617\n","Step=48999 loss=2.37401743070631e-07, accuracy=1.0\n","Test loss = 10.595383644104004, Test accuracy = 0.7536231875419617\n","Step=49099 loss=2.5078444454429416e-07, accuracy=1.0\n","Test loss = 10.593840599060059, Test accuracy = 0.7536231875419617\n","Step=49199 loss=2.5544123160692323e-07, accuracy=1.0\n","Test loss = 10.591632843017578, Test accuracy = 0.7536231875419617\n","Step=49299 loss=2.518837570453236e-07, accuracy=1.0\n","Test loss = 10.589210510253906, Test accuracy = 0.7536231875419617\n","Step=49399 loss=2.2114117905758235e-07, accuracy=1.0\n","Test loss = 10.58651065826416, Test accuracy = 0.7536231875419617\n","Step=49499 loss=2.303799787428673e-07, accuracy=1.0\n","Test loss = 10.583451271057129, Test accuracy = 0.7550724744796753\n","Step=49599 loss=1.808804645397899e-07, accuracy=1.0\n","Test loss = 10.581305503845215, Test accuracy = 0.7550724744796753\n","Step=49699 loss=1.9824949916702473e-07, accuracy=1.0\n","Test loss = 10.57936954498291, Test accuracy = 0.7536231875419617\n","Step=49799 loss=1.8720419968865088e-07, accuracy=1.0\n","Test loss = 10.577054023742676, Test accuracy = 0.7536231875419617\n","Step=49899 loss=1.9255916106408754e-07, accuracy=1.0\n","Test loss = 10.575054168701172, Test accuracy = 0.7536231875419617\n","Step=49999 loss=1.9266158256314726e-07, accuracy=1.0\n","Test loss = 10.57398796081543, Test accuracy = 0.7536231875419617\n","Reached 50001 epochs for CNN\n","[[107 147]\n"," [ 23 413]]\n","Normalized confusion matrix\n","[[0.42125984 0.57874016]\n"," [0.05275229 0.94724771]]\n"],"name":"stdout"},{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAVEAAAEmCAYAAADbUaM7AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nERWSRCMiKiSJRkRUSBKNiKiQJBoRUeEAzqkmcrtYGjgAAAAASUVORK5CYII=\n","text/plain":["<Figure size 432x288 with 2 Axes>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAXQAAAEICAYAAABPgw/pAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n/x9JXq6GdkWmWgAAAABJRU5ErkJggg==\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[]}}]},{"cell_type":"code","metadata":{"id":"XQ1veEV6WZw7","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"JNoPLrG2lB00","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZW4xLrHWvDxe","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}